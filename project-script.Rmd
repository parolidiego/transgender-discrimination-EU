---
title: "Analyzing support for transgenders' rights across the EU"
subtitle: "Survey Research Methodology II Assignment"
author: "Candela Gómez Blanco, Irene García-Espantaleón Artal, Diego Paroli, Bradley McKenzie"
date: "`r Sys.Date()`"
output:
  rmdformats::robobook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Loading libraries

```{r, message = FALSE, warning=FALSE}
rm(list = ls())
library(MASS)
library(performance)
library(tidyverse)
library(haven)
library(readxl)
library(xml2)
library(rvest)
library(janitor)
library(DataExplorer)
library(countrycode)
library(explore)
library(lme4)
library(lmerTest)
library(broom.mixed)
library(ggplot2)
library(e1071)
library(dplyr)
library(scales)
library(tidyr)
library(kableExtra)
library(gmodels)
library(flexplot)
library(corrplot)
library(DescTools)
library(caret)
library(ROSE)
library(randomForest)
library(pROC)
library(mice)
library(glmnet)
library(fastDummies)
library(rempsyc)
library(gbm)
```

# Loading data

## Survey data

```{r}
data_raw <- read_dta("data/ZA7575.dta")
```

## Country-level datasets

This dataframe serves us to join various country-level datasets together later.

```{r, message=FALSE}
# building a df with country names and codes for the EU-28
codelist <- countrycode::codelist |> 
  select(country.name.en, iso.name.en, un.name.en, cow.name, ecb, eurostat, iso2c, iso3c, eu28) |> 
  filter(!is.na(eu28)) 
```

### GDP per capita

This is GDP per capita, PPP (constant 2021 international \$). Retrieved from the World Bank (<https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD>) for the year 2019.

```{r, message=FALSE}
gdp_data <- read_csv("data/gdp_pc_ppp_2021.csv", skip = 4, show_col_types = FALSE)

head(gdp_data)

gdp_data <- gdp_data |> 
  select(`Country Name`, `Country Code`, `2019`)


gdp_data <- gdp_data |> 
  rename(country_name = `Country Name`,
         country_code = `Country Code`,
         gdp_pc_ppp = `2019`)

gdp_data <- gdp_data |> 
inner_join(select(codelist, iso3c), by = c("country_code" = "iso3c"))

str(gdp_data)
```

We now have 28 observations including the UK.

### LGBT rights

Found this LGBT rights index on Our World in Data
(<https://ourworldindata.org/grapher/lgbt-rights-index>) which captures
whether LGBT+ people enjoy the same rights as cisgender people combining
information on 18 different policies. It includes the legal status of
same-sex marriage.

```{r}
lgbt_rights_index <- read.csv("data/lgbt-rights-index.csv")

lgbt_rights_index <- lgbt_rights_index |> 
  filter(Year == 2019) |> 
  inner_join(select(codelist, iso3c), by = c("Code" = "iso3c")) |> 
  select(-Year) # all observations are from 2019

str(lgbt_rights_index)
```

Column names can be renamed:

```{r}
lgbt_rights_index <- lgbt_rights_index |> 
  rename(country_name = Entity,
         country_code = Code,
         lgbt_policy_index = LGBT..Policy.Index) 
```

### Gender inequality index

Gender Development and Gender Inequality indexes, developed by the
United Nations Development. Retrieved from
<https://hdr.undp.org/data-center/documentation-and-downloads> for the
year 2019.

```{r}
gender_index <- read_xlsx("data/UNDP_gender_indexes.xlsx")

gender_index <- gender_index |>
  select(-dimension, -note, -year) |> # empty/useless columns
  inner_join(select(codelist, iso3c), by = c("countryIsoCode" = "iso3c"))

gender_index |> 
  distinct(country) |> 
  nrow()

str(gender_index)
```

We checked that we have indeed 28 distinct countries in the dataset.

This dataset collects many indicators apart from the indexes values. We
are selecting only the Gender Development and Gender Inequality indexes
(GDI and GII):

```{r}
gender_index <- gender_index |> 
  filter(indicatorCode %in% c("gdi", "gii")) |> 
  select(-c(indexCode, indicatorCode, indicator)) |> # removing redundant columns
  pivot_wider(names_from = index, values_from = value)

colnames(gender_index) <- janitor::make_clean_names(colnames(gender_index)) # cleaning spaces and upper cases
```

And then exploring them to see which one is a better fit for modeling
and predictions:

```{r}
summary(gender_index$gender_inequality_index)
summary(gender_index$gender_development_index)

sd(gender_index$gender_inequality_index)
sd(gender_index$gender_development_index)
```

We observe that the second one (Gender Development Index) has a very
small range, indicating that most countries have nearly identical
scores. This is supported by the standard deviation, which shows that
the GII is more spread out than the GDI. Being almost constant, GDI
won’t add much value to the analysis. This is likely due to the fact
that these indexes are created by the United Nations Development
Programme for all countries, so it may not capture the finer differences
between EU countries.

```{r}
cor(gender_index$gender_inequality_index, 
    gender_index$gender_development_index, 
    use = "complete.obs")
```

Surprisingly, the two indexes have a very weak positive correlation.
While they are not inverse, we interpreted them as measuring opposite
things, and had expected a negative correlation. Maybe the correlation
coefficient is not useful here because of the low variation of both variables
(specially GDI), so ultimately we chose to keep GII and discard GDI.

```{r}
gender_index <- gender_index |> 
  select(-gender_development_index)
```

### Economist's Democracy Index

```{r}
democracy_index <- read_xlsx("data/EIU_democracy_index.xlsx", sheet = 4)

# the ISO codes were lowercase which impedes the join
democracy_index$geo <- toupper(democracy_index$geo)

# filter for 2019 and EU28 countries
democracy_index <- democracy_index |> 
  filter(time == 2019) |> 
  inner_join((select(codelist, iso3c)), by = c("geo" = "iso3c"))

# clean var names
names(democracy_index) <- names(democracy_index) %>%
  janitor::make_clean_names() %>%
  gsub("_eiu$", "", .)

democracy_index <- democracy_index |> 
  rename(country_code = geo,
         country_name = name,
         year = time)

str(democracy_index)
```

In this case, we are only keeping the overall index value.

```{r}
democracy_index <- democracy_index |> 
  select(country_name, country_code, democracy_index)
```

### Joining all country-level data together

```{r}
country_level_data <- codelist |> 
  select(iso3c, iso2c) |> 
  left_join(gdp_data, by = c("iso3c" = "country_code")) |> 
  # left_join(rural_data, by = c("iso3c" = "country_code")) |> rural population data will probably be discarded
  left_join(gender_index, by = c("iso3c" = "country_iso_code")) |> 
  left_join(lgbt_rights_index, by = c("iso3c" = "country_code")) |> 
  left_join(democracy_index, by = c("iso3c" = "country_code")) |> 
  select(-contains("country_")) # removing all duplicated country_name columns that were joined from each data frame
str(country_level_data)
```

# Data cleaning

## Selecting relevant variables

We are discarding all variables that relate to trade and globalization
as deemed not relevant for our analysis `qa`. We are also discarding
variables related to energy policies `qb`.

We have also not considered questions specifically about Roma ex `qc8`
and `qc14` and `qc16`

<br>

Some of the doubts we had:
This section will perform initial analysis of variables and there relationship to the target. A value of 1 on the y-axis represents 100% support for transgender rights to change civil papers, a value of 2 represents 100% opposition to the question. Don't knows are removed from this section. These are quick graphs that served use in variable selection, not meant to be graphically appealing.

AGE

```{r, warning=FALSE}
x_df <- data_raw |> filter(qc19<3) |> summarise(mean = mean(qc19), .by = d11)
ggplot(x_df, aes(x = d11, y = mean)) + geom_point() + geom_smooth()
```

As relationship seems pretty linear we are going to use the continuous variable for age. Variance increases at high ages due to fewer respondents.  

<br>

POLITICAL OPINIONS

```{r, warning=FALSE}
x_df <- data_raw |> filter(qc19<3) |> summarise(mean = mean(qc19), .by = d1)
ggplot(x_df, aes(x = d1, y = mean)) + geom_point() + geom_smooth() + xlim(1,10) 
```

For political opinions it seems that the relationship is less linear so we will work on the categories variable. Indeed, there seems to be a clear group (1to4) which is the same used in the variable with 3 categories (left, center and right so we will use the 3-categories variable.

<br>

MARRIAGE

```{r, warning=FALSE}
x_df <- data_raw |> filter(qc19<3) |> summarise(mean = mean(qc19), .by = d7r1)
ggplot(x_df, aes(x = d7r1, y = mean)) + geom_point() + xlim(1,5) 
```

Of all the possible combinations this one seems the best to highlight
differences in our target variable

<br>

NATIONALITY

There is no easy way to create an immigrant dummy by checking whether
someone does not have the nationality of the country in which he is
being interviewed for the survey. So we are just going to use `q1_29` as
a dummy for whether someone owns a non-EU nationality

<br>

POLITICAL INTEREST

Using only `polintr` as a summary of the whole `d71` question about how
strong is your interest in politics in various domains

<br>

EXPERIENCED DISCRIMINATION

For the questions about whether you have experienced discrimination
`qc2_` we are keeping only `qc2_15` which is a binary on whether you have
experienced any discrimination or not. We do not include the other
categories that allowed us to understand if you had experienced
discrimination on the basis of a particular motive. There are data quality 
issues with these variables and we can infer that if a person is a part of a 
specific minority (info from `sd2`) and is being discriminated, it is due to 
that minority status (at least in most cases).

<br>

PERCIEVED DISCRIMINATION in country

This is a question (`qc1`) about how widespread you perceived
discrimination is in your country. It is a rather peculiar question
because it asks individuals for perception at the country level. 
Due to the nature of mixed modelling, we have prioritised using the EU survey 
to create individual level discrimination measures. The country level measures
are captured in that country level data.

For the same reason I am also discarding `qc4` that asks whether in your
country you feel like candidates with certain characteristics would be
at a disadvantage in the recruitment process. It still basically asks
about perceived discrimination at the country level.

And the same reasoning also applies to `qc7` which asks if efforts
towards reducing discrimination are effective in your country.

<br>

HOW DISCRIMINATORY ARE YOU SCORE

We use `qc12` and `qc13` to build a score from 1 to 10 of how
discriminatory are you against certain minorities. We build the
score for each minority and then we can decide later if some are
irrelevant.

Note for some categories this score is a bit stupid (ex. voting for
whether you would feel uncomfortable if your child was in a love
relationship with a old person would give you a high discrimination score
against old people, this is an example of some of the categories for
which it is worth excluding the index)

We aggregate these into scores for discrimination (e.g.religious/ethnic score 
etc.). When required, we can further aggregate to obtain just one score 
measuring individual discrimination. 

`qc6` also asks about discriminatory behaviors, but is about elected
public official, it does not ask about a situation that impacts you
personally. It also uses different categories for minorities therefore I
prefer to use `qc12` and `qc13` rather than `qc6`.

<br>

SUPPORTIVE OF LGBTQ RIGHTS INDEX

Using `qc15` we can again build an index for how supportive of lgbtq
rights a person is.

We again choose to take the mean across the different answers but we
could also use median/mode if we think is better

We could also use `qc18` to try to capture anti lgbtq sentiment, but I
preferred `qc15` as it seemed more straightforward (I therefore deleted
`qc18`, but we can put it back if useful).

<br>

MY VOICE COUNTS

We have used this as a proxy for social alienation. We hypothesised that people 
who feel alienated from society and politics might be less likely to support 
lgbtq rights so we took the mean of the two subquestions in `d72`, which
ask whether you felt like your voice mattered.

<br>

OTHERS

`qc11` (willingness to provide information) is a strange question feels useless to me, I removed it.

I also removed `qc9` as I don't really know what to do with it and
`qc17`.

Here we select our initial variables of interest from the above discussion. 

```{r}
data <- data_raw |> 
  select(serialid, # unique identifier
         isocntry, # 2 digit country code
         d11, # age variables
         q1_29, # nationality of interviewee. options given: EU28+Other+DK, using only other = outside of EU
         d70, #life satisfaction
         polintr, # political interest index (summarizes d71 questions)
         starts_with("sd1"), # friends that are minority groups
         starts_with("sd2"), # are you part of a minority
         sd3, # religion
         qc2_15, # experienced discrimination yourself
         qc3, # where discrimination took place
         starts_with("qc5"), # actions against discrimination
         qc10, # how would you report discrimination
         starts_with("qc12"), # feelings about colleagues being minority
         starts_with("qc13"), # feelings about kid being in a relationship with minority
         starts_with("qc15"), # opinions about lgbtqi 
         qc19, # target variable transgender
         qc20, # non-binary genders in documents
         d1r1, # political ideology
         d7r1, # marital status
         d10, # gender binary
         d8, # eduaction 
         d15a_r2, # current occupation (discarded previous occupation d15b)
         d25, # rural vs urban
         d43t, # phones availiabilty
         d60, # financial stress (paying bills)
         netuse, # internet index
         d63, # social class
         starts_with("d72"), # my voice counts
  )

paradata <- data_raw |> 
  select(serialid, # to match it with the other data
         p2, p3, p3r, p4, p5) # paradata)
```

Removing some of the disaggregrated questions

`sd2_7` to `sd2_10`, these are possible answer deemed irrelevant to the
question about yourself being part of the following minorities: `sd2_7`
other minorities (I don't know what other relevant minorities could be
there), `sd2_8` not part of minorities (can be deducted from the rest),
`sd2_9` refusal to respond, `sd2_10` don't know answers, `sd2t` summary binary
variable for being part of any minority (we are going to keep the more
specific one)

I also remove `qc12_nr` as I prefer to work on the full variable instead
of the recoded version with less categories. Same for `qc13` and `qc18`

```{r}
data <- data |> 
  select(-c(sd2_7, sd2_8, sd2_9, sd2_10, sd2t)) |> 
  select(-(starts_with("qc12") & ends_with("r"))) |> 
  select(-(starts_with("qc13") & ends_with("r"))) |> 
  select(-(starts_with("qc18") & ends_with("r")))
```

## Check overall data quality

```{r}
explore_tbl(data)
plot_intro(data)
```

All columns are numeric columns, the only one which is not is `isocntry`:

```{r}
data |> 
  select(where(~ !is.numeric(.)))
```

Most columns do not have explicit NAs:

```{r, fig.height=6}
plot_missing(data)
```

### Extract data labels

Exploiting the fact that the .dta files has attributes (labels) for all
its columns.

If we search for `attr(colname, "label")` you get back the original long
name of the variable

If we instead search for `attr(colname, "labels")` you get back all the
possible encoding levels of the variable (ex.1,2,3,4 etc.) and by doing
`names(attr(colname, "labels"))` you get back the actual meaning of
those numbers (ex. 1 = "Yes", 2 = "No", etc.)

Will now look into the labels for the different levels that our factor
variables can take:

```{r}
attr(data$d70, "labels")
names(attr(data$d70, "labels"))
tibble(name_labels = names(attr(data$d70, "labels")),
       labels = attr(data$d70, "labels"))

# Create a list of tibbles containing the labels and their associated name for each variable
list_label_tibbles <- 
  #Applies a function across all columns of a df and returns results as a list
  lapply(names(data), function(col_name) {
    labels <- attr(data[[col_name]], "labels")  # Extract labels
    name_labels <- names(labels)  # Extract label names
    # Create tibble with the extracted data only if labels exist
    if (!is.null(labels)) {
      tibble(name_labels = name_labels, labels = labels)} 
    else {NULL}  # Returns a NULL element for columns without labels
  })

# Giving to each element of the list as name the name of the variable
list_label_tibbles <- setNames(list_label_tibbles, names(data))

# For example
list_label_tibbles$d70
```

Right column is what appears in our data (as a number). Left column
is the label that we must assign to that number when we factorize

## Cleaning

### Initial cleaning and pre-processing

Recoding together Germany East and West because we are running analysis at the country level and they are coded separately.

```{r}
unique(data$isocntry)
data <- data |> 
  mutate(isocntry = case_when(
    isocntry %in% c("DE-W", "DE-E") ~ "DE",
    TRUE ~ isocntry))
# We might want to join the full name of the countries using the codelist df
```

<br>

### Recode all factor NA's

Separating variables for which we can use the attribute labels to
factorize them and the variables for which this strategy cannot be used

```{r}
data <- data |> 
  rename(friends_trans = sd1_7)

non_factor_variables <- c("serialid", "tnscntry", "isocntry", "d11", "q1_29", 
                          names(data)[startsWith(names(data), "sd1_")],
                          names(data)[startsWith(names(data), "sd2_")],
                          names(data)[startsWith(names(data), "qc5_")],
                          names(data)[startsWith(names(data), "qc12_")],
                          names(data)[startsWith(names(data), "qc13_")],
                          names(data)[startsWith(names(data), "qc15_")],
                          names(data)[startsWith(names(data), "d72_")],
                          "d8", "opls")
factor_variables <- setdiff(names(data), non_factor_variables)
```

Correctly encoding the factor variables that do not need any further
cleaning

```{r}
# Converting them to factors and assign them their labels automatically
data <- data |> 
  mutate(across(all_of(factor_variables), labelled::to_factor))

# Turning DK into NAs for all the factor variables
data <- data |> 
  mutate(across(all_of(factor_variables), ~ fct_na_level_to_value(., extra_levels = "DK")))

# Converting to numeric the variables that should be numeric
data <- data |> 
  mutate(age = as.numeric(d11),
         years_edu = as.numeric(d8)) |> 
  # Recoding correctly d8 education variable according to unique(data_raw$d11))
  mutate(years_edu = case_when(
    years_edu %in% c(0, 99) ~ NA, # Refusal and DK as NAs
    years_edu == 97 ~ 0, # No full time education = 0
    years_edu == 98 ~ age, # still studying = age
    TRUE ~ years_edu)) |> 
  select(-c(d11, d8))

# Converting q1_29 to factor without assigning labels (1 if non-Eu nationality, 0 if EU nationality)
# Same for sd2_
data <- data |> 
  mutate(nonEU_national = as.factor(q1_29),
         across(starts_with("sd2_"), ~ as.factor(.x))) |> 
  select(-q1_29)
```

<br>

### Create new individual level measures

Dealing with the other variables on which we do some further
pre-processing (feature engineering).

Creating a variable that counts the number of different minority groups a person has acquaintances with:

```{r}
data <- data |>  
  mutate(across(starts_with("sd1"), ~ if_else(.x == 1, 1, 0))) |> 
  mutate(n_friends_minorities = sd1_1+sd1_2+sd1_3+sd1_4+sd1_5+sd1_6+sd1_8) |> 
  relocate(n_friends_minorities, .before="sd1_1") |> 
  select(-starts_with("sd1"))
```

Creating a variable that counts the number of actions against discrimination that you have taken in the last year:

```{r}
data <- data |>  
  mutate(across(starts_with("qc5"), ~ if_else(.x == 1, 1, 0))) |> 
  mutate(n_actions_against_discri = qc5_1+qc5_2+qc5_3+qc5_4) |> 
  relocate(n_actions_against_discri, .after="qc3") |> 
  select(-starts_with("qc5"))
```

Building a discriminatory score:

```{r}
data <- data |> 
  # Coding as NAs "it depends" and "DK"
  mutate(across(starts_with("qc12"), ~ if_else(.x >= 12, NA, .x))) |> 
  mutate(across(starts_with("qc13"), ~ if_else(.x >= 12, NA, .x))) |> 
  # Coding as 5 responses = indifferent
  mutate(across(starts_with("qc12"), ~ if_else(.x == 11, 5, .x))) |> 
  mutate(across(starts_with("qc13"), ~ if_else(.x == 11, 5, .x))) |> 
  # Modifying such that higher is more discriminatory
  mutate(roma_discri = 11 - rowMeans(cbind(qc12_1, qc13_1), na.rm = TRUE),
         black_discri = 11 - rowMeans(cbind(qc12_2, qc13_2), na.rm = TRUE),
         asian_discri = 11 - rowMeans(cbind(qc12_3, qc13_3), na.rm = TRUE),
         white_discri = 11 - rowMeans(cbind(qc12_4, qc13_4), na.rm = TRUE),
         jewish_discri = 11 - rowMeans(cbind(qc12_5, qc13_5), na.rm = TRUE),
         muslim_discri = 11 - rowMeans(cbind(qc12_6, qc13_6), na.rm = TRUE),
         buddihst_discri = 11 - rowMeans(cbind(qc12_7, qc13_7), na.rm = TRUE),
         christian_discri = 11 - rowMeans(cbind(qc12_8, qc13_8), na.rm = TRUE),
         atheist_discri = 11 - rowMeans(cbind(qc12_9, qc13_9), na.rm = TRUE),
         lgb_discri = 11 - rowMeans(cbind(qc12_10, qc13_10), na.rm = TRUE),
         trans_discri = 11 - rowMeans(cbind(qc12_11, qc13_11), na.rm = TRUE),
         intersex_discri = 11 - rowMeans(cbind(qc12_12, qc13_12), na.rm = TRUE),
         disability_discri = 11 - rowMeans(cbind(qc12_13, qc13_13), na.rm = TRUE),
         young_discri = 11 - rowMeans(cbind(qc12_14, qc13_14), na.rm = TRUE),
         old_discri = 11 - rowMeans(cbind(qc12_15, qc13_15), na.rm = TRUE)) |> 
  select(-starts_with("qc12")) |> 
  select(-starts_with("qc13"))
```

We delete the discrimination index against young and old people. This is because one of the question is: how comfortable you would feel if one of your children was in a love relationship with a person from group x. We don't believe these are relevant to our target or discrimination score. 

```{r}
data <- data |> 
  select(-c("old_discri", "young_discri"))
```

Building a score of how supportive or anti lbtq rights you are:

```{r}
data <- data |> 
  mutate(across(starts_with("qc15"), ~ if_else(.x == 5, NA, .x))) |>
  mutate(antilgbtq_rights = round(rowMeans(cbind(qc15_1, qc15_2, qc15_3), na.rm = TRUE), 2)) |> 
  select(-starts_with("qc15"))
# Scale of 1 to 4, 1 = supportive, 4 = homophobic
```

Aggregating together questions on whether you think your voice counts:

```{r}
data <- data |> 
  mutate(across(starts_with("d72"), ~ if_else(.x > 4, NA, .x))) |>
  mutate(social_alienation = rowMeans(cbind(d72_1, d72_2), na.rm = TRUE)) |> 
  select(-starts_with("d72"))
# The higher the more people think their voice does not matter
```

This should be our final selection of variables.

We still need to rename them appropriately and check that all the NAs
are correctly encoded by looking at the summary.

### Further cleaning

Rename columns appropriately

```{r}
data <- data |> 
  rename(
    country = isocntry,
    life_sat = d70,
    ethnic_minority = sd2_1,
    skincolor_minority = sd2_2,
    religious_minority = sd2_3,
    roma_minority = sd2_4,
    sexual_minority = sd2_5,
    disability_minority = sd2_6,
    religion = sd3,
    disc = qc2_15,
    disc_where = qc3,
    disc_contact = qc10,
    trans_docs = qc19,
    gender_docs = qc20,
    left_right = d1r1,
    marital_status = d7r1,
    gender = d10,
    occupation = d15a_r2,
    community = d25,
    phone_access = d43t,
    bill_issues = d60,
    internet_use = netuse,
    social_class = d63
  ) 
```

Now, we will transform the variable `disc`, a variable storing whether you have been subject to discrimination,  which was originally coded in a negative way (1 = "Not mentioned", 2 = "No, you haven’t been discriminated against"), into a positive dummy variable to make its interpretation more straightforward. The variable is 1 if a person has been subject to discrimination:

```{r}
data <- data |> 
  mutate(suffered_discr = as.factor(ifelse(disc == "Not mentioned", 1, 0))) |> 
  select(-disc) |> 
  relocate(suffered_discr, .before = disc_where)
```

Move the variables around to have a ordered dataframe, and assign labels to help interpretation:

```{r}
# Relocating to have a ordered df
data <- data |> 
  relocate(c("age", "gender", "years_edu","community", "marital_status", "occupation", "social_class", "religion", "nonEU_national", "phone_access", "bill_issues", "internet_use"), .after = country) |> 
  relocate(c("left_right", "social_alienation"), .after = polintr) |> 
  relocate(c("friends_trans", "n_friends_minorities", "n_actions_against_discri"), .after = gender_docs)

# Assigning labels to columns which have a difficult meaning
attr(data$gender, "label") <- NULL
attr(data$nonEU_national, "label") <- "OWNS A NON-EU NATIONALITY"
attr(data$social_alienation, "label") <- "HIGHER -> THINK THEIR VOICE DOESN'T  MATTER"
attr(data$ethnic_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$skincolor_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$religious_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$roma_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$sexual_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$disability_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$disability_minority, "label") <- "ARE YOU PART OF X MINORITY" 
attr(data$disability_minority, "label") <- "ARE YOU PART OF X MINORITY"
attr(data$suffered_discr, "label") <- "HAVE YOU BEEN SUBJECT TO DISCRIMINATION"
attr(data$n_friends_minorities, "label") <- "YOU KNOW PEOPLE FROM # NUMBER OF DIFFERENT MINORITES"
attr(data$n_actions_against_discri, "label") <- "YOU HAVE DONE # NUMBER OF DIFFERENT ACTIONS TO FIGHT DISCRIMINATIONS"
attr(data$roma_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$black_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$asian_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$white_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$jewish_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$muslim_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$buddihst_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$christian_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$atheist_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$atheist_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$lgb_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$trans_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$intersex_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$disability_discri, "label") <- "HOW DISCRIMINATORY ARE YOU AGAINST X"
attr(data$antilgbtq_rights, "label") <- "HIGHER -> THEY OPPOSE MORE RIGHTS TO LGBTQ"
```

This is our dataset so far:

```{r}
summary(data)
```

Our dataset seems quite balanced across the different countries

```{r}
data |> count(country)
```

Although some levels of occupation have more observations than others,
we have enough observations for each level so we do not need to
aggregate over different levels for the variable `occupation`

```{r}
data |> count(occupation)
```

Very few people declare themselves to be the higher class of society,
nonetheless we keep this level because it makes sense.

```{r}
data |> count(social_class)
```

Given the low number of observations in some categories of the variable 
`religion` and the high number of categories we opt to aggregate some of
them, otherwise we risk too much noise in our models.

```{r}
data |> count(religion)

# Checking if the means of groups we are going to aggregate are similar
data |> 
  mutate(trans_docs=as.numeric(trans_docs)) |> 
  summarise(mean = mean(trans_docs, na.rm=TRUE), .by = religion) |>
  arrange(mean)
```

We are going to group together atheist with agnostic. We are also going to put sikh, buddhists, jewish and hindu into the other category because there are only very few observations. Finally we are going to group together all muslims.

```{r}
data <- data |> 
  mutate(religion = fct_collapse(religion,
                                "Non-believers" = c("Atheist", "Non believer or agnostic"),
                                "Other" = c("Sikh", "Buddhist", "Jewish", "Hindu", "Other"),
                                "Muslim" = c("Muslim - Shia", "Muslim - Sunni", "Other Muslim")))

#This is what we end up with
data |> count(religion)
```

### Recoding missing values

In this section we will convert all levels of factors that are redundant to NAs.

First we will substitute de "Refusal" answers in the remaining variables as NAs. If the respondent refuses to answer, we treat is as equivalent to not knowing his or her answer. We exclude `friends_trans` because for that category it could be worth analyzing refusals.

```{r}
factor_variables <- names(data)[sapply(data, is.factor)]

data <- data %>%
  mutate(across(
    all_of(setdiff(factor_variables, "friends_trans")),  # Exclude "friends_trans"
    ~ {
      # Look for levels that contain "REFUSAL"
      refusal_levels <- grep("REFUSAL", levels(.), value = TRUE, ignore.case = TRUE)
      # If there are levels containing "REFUSAL"
      if (length(refusal_levels) > 0) { 
        # Convert in NA
        fct_recode(., NULL = refusal_levels)  
      } 
      # if not remain without changes
      else {
        .  
      }
    }
  ))
```

Mutate NONE level of `social_class` to NA.

We also turn `marital_status`'s level OTHER into NAs, as it is difficult to give it any other meaning. Same for social class

```{r}
data <- data %>%
  mutate(social_class = fct_recode(social_class, NULL = "None (SPONTANEOUS)"))

data <- data %>%
  mutate(marital_status = fct_recode(marital_status, NULL = "Other (SPONT.)"),
         social_class = fct_recode(social_class, NULL = "Other (SPONTANEOUS)"))
```

```{r, fig.height=8}
plot_intro(data)
plot_missing(data)
```

The total number of missing observations is quite low (4%) so missing data should not be a huge problem.

Given the high percentage of missing values we delete `disc_where`. This variable was expected to have a high percentage of missingness as it is a question that applies only to people that have been subject to discrimination. So it is not a variable of huge importance.

The variable with the second highest number of missing data is `left_right`, given the importance of this variable it is probably worth to impute those values.

Next there are the variables `trans_docs` and `gender_docs` which are respectively our target variable and a closely related variable

Then there is `disc_contact` which will be deleted as well, given that it does not add meaningful information to our analysis.

```{r}
data <- data |> 
  select(-c("disc_where", "disc_contact"))
```

# Exploratory Data Analysis 

Plotting the distribution of the numeric variables

```{r, warning=FALSE}
# Identify numeric variables
numeric_vars <- names(data)[sapply(data, is.numeric)]
numeric_vars <- numeric_vars[numeric_vars != "serialid"]

# Creating histogramas y calculating skweness
for (var in numeric_vars) {
  p <- ggplot(data, aes(x = .data[[var]])) +
    geom_histogram(binwidth = 1, fill = "blue", color = "black") +
    labs(title = paste("Histogram of", var), x = var, y = "Count") +
    theme_minimal()
  print(p)}
```

## Analysis of individual-level variables

We must keep in mind our target variable is `qc19` "Do you think that transgender persons should be able to change their civil documents to match their inner gender identity?"

```{r, fig.height=5}
data_percent <- data |>
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |> 
  group_by(trans_docs) |> 
  summarise(count = n()) |> 
  mutate(percentage = count / sum(count) * 100)

ggplot(data_percent, aes(x = trans_docs, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity") +  
  geom_text(aes(label = sprintf("%.1f%%", percentage)),  
            vjust = -0.5, size = 4, color = "black") +  
  scale_y_continuous(labels = scales::percent_format(scale = 1)) + 
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Support for Transgender Rights",
    y = "Percentage"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
```

Taking a look to our variable of interest alone, we see how the 52.7% of
our sample are in favor of trans people to change their gender in their
civil documents. However, there is also a significant opposition
(35,3%), and a 12% who answered "Don't know". We will try to explore
this distribution along the variables we consider to be most important
for our analysis.

### Sociodemographic variables

#### Gender

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |> 
  count(trans_docs, gender, name = "n") |> 
  group_by(gender) |>  
  mutate(percentage = n / sum(n))

ggplot(data_summary, aes(x = gender, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Gender",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

# Create contingency table
contingency_table <- table(data$gender, data$trans_docs)

# Calculate chisquare test
chisq_test <- chisq.test(contingency_table)
print(chisq_test)
```

Men exhibit a lower percentage of favorable or don't know responses and a higher rate of rejection compared to women. There is a statistically significant association between gender and our target variable, being women more supportive

#### Age

```{r}
data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  mutate(age_bin = cut(age, breaks = seq(min(age, na.rm = TRUE), max(age, na.rm = TRUE), by = 10), include.lowest = TRUE)) |> 
  # There are 4 observations above 95 y.o. that do not fall within any of the previously difined bins. Given their low number we drop them
  drop_na(age_bin) |> 
  count(age_bin, trans_docs) |> 
  group_by(age_bin) |> 
  mutate(percentage = n / sum(n) * 100) |>  
  ggplot(aes(x = age_bin, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "stack") +
  coord_flip() +
  labs(x = "Age Bins", y = "Proportion of responses", fill = "Support the right",
       title = "Support for the right of trans people to change their gender in civil documents") +
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  theme_minimal() +
   theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

```

Younger people seem more likely to support the right for transgender people to have their gender changed on official documents. 

We also notice a big increase of NAs for older people, meaning older people are less likely to respond to this question (because of unfamiliarity with the topic maybe).

So how would that distribution look if we get rid of NAs

```{r}
data |> 
  drop_na(trans_docs) |> 
  mutate(age_bin = cut(age, breaks = seq(min(age, na.rm = TRUE), max(age, na.rm = TRUE), by = 10), include.lowest = TRUE)) |> 
  # There are 4 observations above 95 y.o. that do not fall within any of the previously defined bins. Given their low number we drop them
  drop_na(age_bin) |> 
  count(age_bin, trans_docs) |> 
  group_by(age_bin) |> 
  mutate(percentage = n / sum(n) * 100) |>  
  ggplot(aes(x = age_bin, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "stack") +
  coord_flip() +
  labs(x = "Age Bins", y = "Proportion of responses", fill = "Support the right",
       title = "Support for the right of trans people to change their gender in civil documents") +
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  theme_minimal() +
   theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

We see that differences among age groups are not so evident anymore.

#### Religiosity

We have two variables related to religiosity: `religion` (the religious affiliation professed by the respondent) and `religious_minority` (whether or not the respondent belongs to a religious minority group).

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  mutate(religious_minority = fct_recode(religious_minority, "Yes" = "1", "No" = "0")) |> 
  count(trans_docs, religion, name = "n") |> 
  group_by(religion) |>  
  mutate(percentage = n / sum(n))

ggplot(data_summary, aes(x = reorder(religion, ifelse(trans_docs == "Yes", -percentage, 0)), y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Are you part of a religious minority?",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )

# Create contingency table
contingency_table <- table(data$religion, data$trans_docs)

# Calculate chisquare test
chisq_test <- chisq.test(contingency_table)
print(chisq_test)

```
The small p-value confirms that the relationship between `religion` and `trans_docs` is statistically significant. We see that non-believers are most likely to support the rights, while people who are Orthodox Christian and Muslim are most likely to oppose the rights. They are also most likely to not provide a don't know response.

We then run for the religious minority question (sd2_2). This provides us with less clear results. 

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  mutate(religious_minority = fct_recode(religious_minority, "Yes" = "1", "No" = "0")) |> 
  count(trans_docs, religious_minority, name = "n") |> 
  group_by(religious_minority) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = religious_minority, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Are you part of a religious minority?",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

#### Ideology

```{r, warning=FALSE}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  count(trans_docs, left_right, name = "n") |> 
  group_by(left_right) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = left_right, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Political affiliation",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

Stronger left-wing support for trans rights. Right-wing respondents are
the most divided, with high opposition levels. Non-responses are highest
among those without ideological alignment and right leaning respondents. 

#### Social class

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  count(trans_docs, social_class, name = "n") |> 
  group_by(social_class) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = social_class, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change \ntheir gender in civil documents",
    x = "You identify as:",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

Although D63 of the questionnaire measures (oneself's and one's
household's) self-perceived social class, it is still a relevant and
seemingly divisive variable. We observe a positive correlation, where
support grows with increasing (perceived) social class, while explicit
lack of support and no response are higher for lower-class individuals.

#### Education

```{r}
# limit to main age ranges in age completed
data |> 
  mutate(main_educ = ifelse(between(years_edu, 15, 30), 1, 0)) |> 
  group_by(main_educ) |> 
  summarise(count = n(),
            prop = count/nrow(data)*100)

data |> 
  filter(between(years_edu, 15, 30)) |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  count(trans_docs, years_edu, name = "n") |> 
  group_by(years_edu) |>  
  mutate(percentage = n / sum(n)) |> 
  ggplot(aes(x = years_edu, y = percentage, fill = trans_docs)) +
      geom_bar(stat = "identity", position = "fill") +
    geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 3) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Distribution of support by education",
    x = "Age completed education, from 15 to 30 years old",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  theme_minimal()+
  theme(
    panel.grid = element_blank())

```

Limiting the data top only look at support around the main education. Which includes around 87% of the observations, we see a less clear relationship between age finishing education and support. However those who appear to finish education before age 20 tend to have higher opposition. 

#### Area of residence

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  count(trans_docs, community, name = "n") |> 
  group_by(community) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = community, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their \ngender in civil documents",
    x = "Place of residence",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

Support does not vary much by area of residence.

### Other individual-level variables

#### Having trans friends

```{r}
data_summary <- data |> 
  mutate(trans_docs = fct_na_value_to_level(trans_docs, "Don't Know")) |>
  count(trans_docs, friends_trans, name = "n") |> 
  group_by(friends_trans) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = friends_trans, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their \ngender in civil documents",
    x = "Do you know a transgender person?",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

Having trans friends is associated with a higher support, while those
who refuse to disclose their connections to trans people tend to oppose
more.

## Country-level EDA

```{r}
country_support <- data %>%
  mutate(
    trans_docs = case_when(
      is.na(trans_docs) ~ "DK",
      trans_docs == "Yes" ~ "1",
      trans_docs == "No" ~ "2",
      TRUE ~ as.character(trans_docs)
    )
  ) %>%
  group_by(country, trans_docs = trans_docs) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(country) %>%
  mutate(proportion = (count / sum(count))*100) %>%
  ungroup()
```

```{r}
trans_docs_prop <- country_support %>%
  filter(trans_docs == "1") %>%
  select(country, proportion) %>%
  rename(trans_support = proportion)

country_long <- country_level_data %>%
  inner_join(trans_docs_prop, by = c("iso2c" = "country")) |> 
  pivot_longer(cols = c(gdp_pc_ppp, gender_inequality_index, lgbt_policy_index, democracy_index),
    names_to = "variable", values_to = "value")

ggplot(country_long, aes(x = value, y = trans_support)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgray") + # we can remove this
  geom_point(size = 2, color = "blue", alpha = 0.7) +
  geom_text(aes(label = iso3c), vjust = -0.5, hjust = 0.5, size = 3) +
  facet_wrap(~variable, scales = "free_x") +
  labs(
    title = "Relationship between country-level variables and trans support",
    x = "Country-level variable",
    y = "Proportion of Yes answers"
  ) +
  theme_minimal()
```

Democracy and LGBT Policy indeces seem to have a positive linear
relationship with the target variable, while Gender Inequality index has
a negative linear relationship. The relationship with GDP per capita is
non-linear (logarithmic?). There appears to be a "ceiling" of support,
beyond which increases in GDP per capita lose effect.

Boxplots for country-level variables:

```{r}
ggplot(country_long, aes(y = value, x = variable)) +
  geom_boxplot(outlier.color = "red", outlier.shape = 16, fill = "lightblue") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Distribution of country-level variables", x = NULL, y = "value") +
  facet_wrap(~variable, scales = "free") +
  theme(axis.text.y = element_blank(),  
        axis.ticks.x = element_blank())
```

```{r}
country_level_data_cor <- country_level_data %>%
  inner_join(trans_docs_prop, by = c("iso2c" = "country"))

cor_matrix <- cor(country_level_data_cor %>%
                    select(gdp_pc_ppp, democracy_index, gender_inequality_index, 
                           lgbt_policy_index, trans_support),
                  use = "complete.obs")

corrplot(cor_matrix, method = "color", type = "upper", 
         addCoef.col = "black", tl.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))
```

Some heavy correlations which is a little worrying, but as we are going to use 
these data to build cross-level interactions, this should not compromise the 
reliability of our analysis too much.


## Paradata

```{r}
# Convert into factor
paradata <- paradata %>%
  mutate(across(where(~ !is.null(attr(., "labels"))), labelled::to_factor))

# Merging datasets for the analysis
merged_paradata <- merge(data, paradata, by = "serialid", all.x = TRUE)
```

### Number of persons present during interview

```{r}
library(gmodels)

# Convert NA in a explicit category
merged_paradata$trans_docs <- as.character(merged_paradata$trans_docs)   
merged_paradata$trans_docs[is.na(merged_paradata$trans_docs)] <- "DK"  
merged_paradata$trans_docs <- as.factor(merged_paradata$trans_docs)  

# CrossTable
CrossTable(merged_paradata$p4, merged_paradata$trans_docs,
           digits=2, 
           expected=F, 
           asresid=T, 
           chisq=TRUE, 
           prop.chisq=F, 
           format="SPSS")

```

The table shows a significant association between the number of people present during the interview and support for transgender people to change their documents. The actual support does not vary too much though, they are all within 3% of each other.  

### Duration of interview

Regarding the duration of the interview we have 2 options `p3` and `p3r`. We will use `p3r` to have a simpler analysis

```{r}
# CrossTable
CrossTable(merged_paradata$p3r, merged_paradata$trans_docs,
           digits=2, 
           expected=F, 
           asresid=T, 
           chisq=TRUE, 
           prop.chisq=F, 
           format="SPSS")

```

It appears that the longer the interview, the more likely to support the people are. The highest support being 75-89 minutes while the highest opposition and and highest DK were in <15 minute interviews. 

### Respondent cooperation

```{r}
data_summary <- merged_paradata |> 
  mutate(trans_docs = fct_recode(trans_docs, "Don't Know" = "DK"))|>
  count(p5, trans_docs, name = "n") |> 
  group_by(p5) |>  
  mutate(percentage = n / sum(n))  

ggplot(data_summary, aes(x = p5, y = percentage, fill = trans_docs)) +
  geom_bar(stat = "identity", position = "fill") +
  geom_text(aes(label = scales::percent(percentage, accuracy = 1)),  
            position = position_stack(vjust = 0.5), size = 4) + 
  scale_y_continuous(labels = percent_format()) + 
  scale_fill_manual(
    values = c("Yes" = "#2ECC71", "No" = "#E74C3C", "Don't Know" = "#3498DB")
  ) +
  labs(
    title = "Support for the right of trans people to change their gender in civil documents",
    x = "Respondent cooperation in the interview",
    y = "Proportion of Responses",
    fill = "Support the right"
  ) +
  coord_flip() +  
  theme_minimal()+
  theme(
    panel.grid = element_blank(),
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank()
  )
```

Respondents who engaged better in the survey were more likely to express
support. This is the most clear paradata relationship. 

Low cooperation had higher DK response and opposition to trans documents support. 

# Correlation and multicollinearity

Now we want to take a look at the possible multicollinearity of our explanatory
variables. For it, we calculate the correlation of our numerical variables. As
we have too many variables, we visualize just those with a strong correlation to 
focus on key dependencies.

```{r}

cor_matrix <- cor(data %>%
                    select_if(is.numeric) %>%
                    select(-serialid),
                  use = "complete.obs")

corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))


# Create a matrix where only correlations >= 0.7 are maintained
cor_filtered <- ifelse(abs(cor_matrix) >= 0.7, cor_matrix, NA)

# Graph
corrplot(cor_filtered, method = "color", type = "upper", 
         tl.col = "black", 
         col = colorRampPalette(c("blue", "white", "red"))(200),
         na.label = " ",
         insig = "blank") 
```

Most strong correlations are among variables related to race, religion or sexual
orientation discrimination. Consequently, we are grouping them into 3 new
composite variables by averaging related ones, in order to reduce multicollinearity.


```{r}
# Creating grouped variables
data <- data %>%
  mutate(
    racial_discri = rowMeans(select(., roma_discri, black_discri, asian_discri), na.rm = TRUE), #no white
    sexual_discri = rowMeans(select(., lgb_discri, trans_discri, intersex_discri), na.rm = TRUE),
    religious_discri = rowMeans(select(., jewish_discri, muslim_discri, buddihst_discri), na.rm = TRUE)
  ) 

# Removing original variables to avoid redundancy
data <- data |> 
  select(-c(roma_discri, black_discri, asian_discri, 
            lgb_discri, trans_discri, intersex_discri, 
            jewish_discri, muslim_discri, buddihst_discri))

# Graphical representation
cor_matrix <- cor(data %>%
                    select_if(is.numeric) %>%
                    select(-serialid),
                  use = "complete.obs")


corrplot(cor_matrix, method = "color", type = "upper", 
         tl.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))
```

We realize our new variables are still highly correlated between them (bottom-right corner). For reducing the dimensionality of the dataset even more while preserving information we create one single variable for general minority discrimination.

```{r}
# Creating grouped variable
data <- data %>%
  mutate(
    minority_discri = rowMeans(select(., racial_discri, sexual_discri,
                                      religious_discri, disability_discri), na.rm = TRUE)) 
```

We decided to exclude `white_discri`, `atheist_discri`, `christian_discri` because we  realized they had a median of 1 and a quite small mean, meaning there is almost no reported discrimination against these groups in the dataset.

```{r}
summary(data$white_discri)
summary(data$atheist_discri)
summary(data$christian_discri)

data <- data |> 
  select(-c(white_discri, atheist_discri, christian_discri, racial_discri, 
            sexual_discri, religious_discri, disability_discri))

```

Final check of the correlation matrix to confirm that multicollinearity has been reduced.

```{r, fig.height=6}
cor_matrix <- cor(data %>%
                    select_if(is.numeric) %>%
                    select(-serialid),
                  use = "complete.obs")


corrplot(cor_matrix, method = "color", type = "upper", 
         addCoef.col = "black", tl.col = "black",
         col = colorRampPalette(c("blue", "white", "red"))(200))
```

# Class imbalance

```{r}
prop.table(table(data$trans_docs, useNA = "ifany"))
prop.table(table(data$trans_docs, useNA = "no"))
```

Our target variable has a 60/40 distribution so we are not worried about class imbalance.

Less represented classes in other variables have already been aggregated if deemed necessary.

# Analysis of missing values

Here we analyse the NA reponses to our target variable. This is a sort
of robustness check to understand our data and learn if there are
patterns in NA response that we should be worried about e.g. if there
are specific demographics of people who are less likely to respond.

Overall the analysis below shows that non response is similar to the
descriptive data and aligned with some groups reported "No" to
supporting trans_docs in `qc19`. So we're more at risk of underreporting
"no" votes in this survey due to non-response. The correlation is low
though so it's not a major concern.

## Who is more likely to not respond to our target? 

There are 3,280 NA responses to the target variable. To try understand
if the missing values are at random, we will test the correlation
between NA response and Use the DK (don't know) for our target variable.

In the descriptive analysis above, we have already seen in raw terms
that NA responses were more frequent from some groups e.g.

-   women,

-   older people,

-   people who also responded NA for political ideology,

-   People who do not have trans friends and those who refused to
    respond to the friendship question had higher NA responses to the
    target variable.

-   People with lower survey cooperation

-   By country, we already understand from the challenge description
    that the NA responses vary. This is a significant disparity.

Create a binary variable for the DK

```{r}
cntry_name <- codelist |> select(iso2c, country_name = country.name.en)
dk_target <- data |> 
  mutate(target_NA = ifelse(is.na(trans_docs), 1, 0)) |> 
  left_join(cntry_name, by = join_by(country == iso2c)) |> 
  dplyr::select(-trans_docs, -serialid, -country)

table(dk_target$target_NA)
```

Confirm DK rates by country. They vary from 1.4% in Belgium to 28.5% in
Bulgaria. Overall, the average is 11.95%.

```{r}
dk_target |> 
  group_by(country_name) |> 
  summarise(count_na = sum(target_NA),
            num_resp = length(country_name),
            pct_na = count_na/num_resp*100) |> 
  ggplot(aes(x=reorder(country_name, -pct_na), y = pct_na))+
  geom_col()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_y_continuous(labels = scales::percent_format(scale = 1)) + 
  labs(y = "Proportion of NA responses")
```

If all NA responses to the target are removed, this is how much the
"Yes" count increases for each of our countries. We see this is not
proportional and the NA vote removal makes our differences in
Yes/No differences appear larger.

```{r}
data |> 
  group_by(country) |> 
  summarise(yes_count = sum(trans_docs == "Yes", na.rm=TRUE),
            num_resp = sum(!is.na(trans_docs)),
            num_na = sum(is.na(trans_docs)),
            pct_yes_withna = yes_count / length(country)*100,
            pct_yes = yes_count/num_resp*100)|> 
  ggplot(aes(x = reorder(country, -pct_yes))) +
  geom_col(aes(y = pct_yes), fill = "red") + # First bar (pct_yes)
  geom_col(aes(y = pct_yes_withna), fill = "yellow", alpha = 0.5) + # Second bar (pct_yes_withna)
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title.x = element_blank()) +
  scale_y_continuous(labels = scales::percent_format(scale = 1),
                     limits = c(0,100)) + 
  labs(title ="Support for transgender rights to legally change documents, with and without NA's")
```

### Check correlation with DK of target

Run model to test what is correlated with DK response, use Cramers V for
association between the factor variables and correlation for the numeric
variables in the cleaned data.

1.  Model the data for DK responses against the factor variables.

```{r}
factor_subset <- dk_target %>%
  dplyr::select(target_NA, where(is.factor)) |> 
  mutate(target_NA = factor(target_NA))

# Run cramers V for factors

target_var <- "target_NA"
factor_variables <- names(factor_subset)
factor_variables <- factor_variables[factor_variables != target_var]

cramers_v_results <- list()

for (variable in factor_variables) {
    contingency_table <- table(factor_subset[[target_var]], 
                               factor_subset[[variable]])
    cramers_v <- CramerV(contingency_table)
    cramers_v_results[[variable]] <- cramers_v
}

# Create a Tibble for Results
cat_results <- tibble(variable = names(cramers_v_results),
       cramers_v = unlist(cramers_v_results)) |> 
  arrange(desc(cramers_v))
cat_results
```

This shows all of the factors have quite low association with the NA
responses. The highest being internet use. This is a good sign that the
missing values are random.

2.  Look at the numeric variables and test correlations:

```{r}
numeric_subset <- dk_target %>%
  dplyr::select(target_NA, where(is.numeric)) 

target_var <- "target_NA"
numeric_variables <- names(numeric_subset)[names(numeric_subset) != target_var]

cor_results <- list()

for (variable in numeric_variables) {
    correlation <- cor(numeric_subset$target_NA, 
                       numeric_subset[[variable]],
                       use = "pairwise.complete.obs")
    cor_results[[variable]] <- correlation
}

# Create a Tibble for Results
cor_results <- tibble(variable = names(cor_results),
       cor = unlist(cor_results)) |> 
  arrange(cor)
cor_results
```

Again, this is not too much cause for concern. We only have around 10%
correlations, positive and negative with the target variable.

### Modelling relationship with key variables and non-response

Here we run a logistic regression to test for the most
important/significant variables. First, we make a subset of data with
only our variables that were most correlated in the seciton above (use
above +/- 9.5% correlation and above 10% cramers V) and also include
gender and country.

```{r}
cor_results |> filter(cor > 0.09 | cor < -0.9) |> pull(variable)
cat_results |> filter(cramers_v >0.1) |> pull(variable)
```

Now model with those variables to see if they are significant in
explaining the DK values. Gender will also be included as a key
variable.

We run a stepwise AIC on these most correlated variables and then use
the best model to test the overall model fit.

```{r}
# first, run stepwise to determine most important 
# run base model 
dk_fit_null <- glm(target_NA ~ 1,
              data = dk_target, 
              family = "binomial")
# run full fit model of most correlated vars with country (also age^2 for good practice)
dk_fit <- glm(target_NA ~ gender + age + I(age*age) + antilgbtq_rights + internet_use + social_class + country_name, 
              data = dk_target, 
              family = "binomial")

# use stepwise to select best mode l
aic_1 <- MASS::stepAIC(dk_fit, scope = list(upper = dk_fit, 
                             lower = dk_fit_null),
        direction = "both", k = 2, trace=0) # forward based on AIC

# filter vars significant at 5% and calculate exponential
broom::tidy(aic_1) |> 
  filter(p.value<0.05) |> 
  mutate(oddsratio = exp(estimate)) |> 
  select(term, oddsratio, p.value)
```

The best model is the one without age but includes age\^2. So we run the
full model to keep the lower level variables included.

The model has some interesting findings to who did not respond to our
findings. Some things we can see with the odds ratios (excluding
interpretation of countries as that is covered above):

-   women are around 20% more likely to give NA responses

-   Anti-lgbti rights people were about 10% more likely to not respond

-   Internet use was significant, those who had not used the internet or did
    not have access, were less likely to respond compared to those who use 
    the internet everyday. 

-   Social class was significant at all levels, this means the working
    class were more likely to NA respond than all other class levels. 

<br>

Below is an option using undersampling (to reduce class imbalance of being a NA versus not being a NA for our target variable).

```{r}
na_model <- glm(target_NA ~ .,
                      data = dk_target, 
                      family = "binomial")

broom::tidy(na_model) |> arrange(desc(estimate))
```

Approach to test relationship to DK with splitting and undersampling:

```{r}
# create new dataset with just vars we want, including age^2
dk_target2 <- dk_target |> 
  select(target_NA, age, antilgbtq_rights, internet_use, social_class, country_name) |> 
  mutate(age_sq = age*age, .after = age)

# to deal with some class imbalance, undersample from the majority group (NA = 0)
set.seed(123)
# Generate an index
index <- createDataPartition(dk_target2$target_NA, p = 0.7, list = FALSE, times = 1)

# Subset the dataframe
train <- dk_target2[index, ]
test <- dk_target2[-index, ]
# check splits 
prop.table(table(train$target_NA))
prop.table(table(test$target_NA))
```

Now undersample the majority

```{r, warning=FALSE}
set.seed(123)
under <- ovun.sample(target_NA~., 
                     data=train, 
                     method = "under", 
                     N = 4000)$data 
    # limit the sample to 4000 obs as we have ~2300 for target minority class. 

table(under$target_NA)

#run a rf model to see how much of the NA values we can explain
rfunder <- randomForest(target_NA~., data=under)
rfunder

# check a simple logistic model oucomes for undersampled data
broom::tidy(glm(target_NA ~., 
                data = under, 
                family="binomial")) |> 
  filter(p.value<0.05) |> 
  mutate(oddsratio = exp(estimate)) |> 
  select(term, oddsratio, p.value)

```

We see that only around 5% of the variance is explained through RF. However The undersampled data with a simplified dataframe finds that higher DK were associated with being anti lgbti rights, less frequent internet use and being working class. This is all useful to compare to overall response rates. 

### Testing the NA values against paradata

Here we extend the model to see if paradata is related to changed
non-response.

```{r}
para <- paradata |> 
  bind_cols(trans_docs = data$trans_docs,
            age = data$age,
            gender = data$gender,
            country = data$country) |> 
  mutate(target_NA = ifelse(is.na(trans_docs), 1, 0)) |> 
  select(-serialid, -trans_docs)

summary(para$target_NA)
str(para)
```

First, again test correlation. We use Cramers V since we only have
factors:

```{r}
target_var <- "target_NA"
para_vars <- names(para)[names(para) != target_var]

cramers_v_results <- list()

for (variable in para_vars) {
    contingency_table <- table(para[[target_var]], 
                               para[[variable]])
    contingency_table_no_dk <- contingency_table[, colnames(contingency_table) != "DK"]
    cramers_v <- CramerV(contingency_table_no_dk) 
    # include this for no DK as it is all zero which creates errors
    cramers_v_results[[variable]] <- cramers_v
}


# Create a Tibble for Results
tibble(variable = names(cramers_v_results),
       cramers_v = unlist(cramers_v_results)) |> 
  arrange(desc(cramers_v))
```

Again, we see low correlation with the paradata and cramers V.

Second, we run another logistic model with just Paradata included.

```{r}
na_para_model <- glm(target_NA ~ p2 + p3r + p4 + p5 + age + I(age*age) + gender + factor(country), 
                     data = para, 
                     family=binomial(link = "logit"))

broom::tidy(na_para_model) |> 
  filter(p.value<0.05) |> 
  mutate(oddsratio = exp(estimate)) |> 
  select(term, oddsratio, p.value)
```

We see that when controlling for age, gender and country, the survey
cooperation variable is significant.

When compared to those with an excellent cooperation rating, people were
more likely to respond. This increases with lacking cooperation. If
cooperation was fair, there was around 40% more chance of NA, if
cooperation was average, around 75% higher chance of NA. If cooperation
was bad, there was about 350% higher chance of a NA response to our
target variable. This variable also has risk of reverse causality, as higher 
NA respose is likely a reason to be rated as low cooperation. 

Overall, this section found that people more likely to respond NA
seems similar to those more likely to oppose trans rights to change papers. 
So there is some risk of under reporting of true support if people opposed are not 
responding. This is likely capturing some social desirability as people don't 
want to appear discriminatory and hide their true views. 

# Imputing missing data

As we have different types of variables we decide to go with the default methods. Selecting the best method for each type of variable would be too computationally intensive.

We know that it would be better to impute multiple datasets and then pool the results of different regressions on those different datasets together, but running more than one multilevel regression would be too computationally intensive and so we are not able to do this.

```{r, message=FALSE}
# Excluding target variable and other useless variables
imputation_data <- data |> 
  select(-c(serialid, country, trans_docs))

# Running it with the default methods
# By default: numerical variables -> pmm, binary factors -> logreg, > 2 levels factors -> polyreg
imputed_data <- mice(imputation_data, m = 1, seed=1234)
imputed_data$method

final_data <- complete(imputed_data)

colSums(is.na(imputation_data))
colSums(is.na(final_data))

final_data <- final_data |> 
  cbind(data$trans_docs) |> 
  cbind(data$country) |>
  rename("trans_docs" = "data$trans_docs",
         "country" = "data$country") |> 
  relocate(country, .before = everything())
```

# Joining together all our data

`country_level_data` stores all our data defined at the country level. 

`final_data` stores all our data (after imputation) for individual level variables.


```{r}
complete_df <- final_data |> 
  left_join(country_level_data, 
            by = join_by(country == iso2c)) |> 
  select(-c(country.y, iso3c))
```

# Explanatory model

Before running our final explanatory model we run a logistic regression with all our individual level predictors to see which variables are significant.

We also run a stepwise regression and Lasso regression to select the most important subset of predictors. 

Comparing the outputs of these 3 regressions can gives us hint about which are the most important variables and levels to include in our final explanatory model.

### Simple logistic regression

I am using only individual level data for now. This is a simple model to see at the global level i.e. no country level control, which variables are significant. The numeric data has also been scaled. 

```{r}
datalr <- final_data |> 
  # Dropping NAs (there should be NAs only in our target variable)
  drop_na() |> 
  # Selecting only predictor variables
  select(-"country") |> 
  # Scale variables 
  mutate(across(where(is.numeric), scale)) |> 
  # Transforming target to binary numeric Yes=1, No=0
  mutate(trans_docs = as.numeric(trans_docs == "Yes"))
  
# now run the full model
simple_logistic <- glm(trans_docs ~ ., data = datalr, family = binomial(link = "logit"))

# Save the summary
logistic_summary <- summary(simple_logistic)

# create a tibble to more easily search through significant variables
logistic_results <- tibble(
  broom::tidy(simple_logistic) |>
    filter(p.value<0.05) |>
    mutate(oddsratio = exp(estimate)) |>
    select(term, estimate, oddsratio, p.value) |> 
    arrange(oddsratio))
  
print(logistic_results, n = Inf)
```

##### Interpeting results

To interpret the coefficients as odds ratios, anything above 1 indicates they are more likely to support the changes to civil documents for trans people. 

The logistic regression shows that 26 statistically significant terms. This includes factors variables with multiple terms. Of our 29 scaled variables, the significant variables (at the 5% level) are:

More likely to OPPPOSE the right to change civil documents for trans people, include:  

- men (compared to women)

- younger people (this is in a way unexpected)

- people who oppose legal rights to add a third-gender in official docs (compared to those who do)

- people who refused to answer whether they have trans friends (compared to those who do)

- people who are more financially sound (have fewer bill issues)

- people not satisfied with their life (compared to those who are 'very satisfied')

- people who identified as roma or an ethnic minority

- people who are more discriminatory against minorities

<br>

More likely to SUPPORT the the right

- older people

- women

- people who were self-employed (compared to all other occupation types incl students)

- unmarried (single) people

- non-believers (religious)

- people with a landline and mobile

- people who use the internet everyday/almost everyday (compared to all other internet use categories)

- people who reported being more left wing (compared to right wing)

- people who had friends in minority groups.

<br>

_NUMERIC VARIABLES_

*Without scaling*, we interpret logistic regression output as: 

`coef(simple_logistic)`: these coefficients represent the change in the log-odds for a one-unit increase in the corresponding independent variable

`exp(coef(simple_logistic))`: the exponential of the slope coefficient (exp(B)) tells us the change of the odds if the independent variable increases by one unit

*If the data is scaled:*

A coefficient B (from a logistic regression) now reflects the change in the log-odds for a one standard deviation increase in the predictor variable, rather than a one-unit increase in the original scale.

The odds ratio (exp(B)) now tells you how the odds change with a one standard deviation increase in the predictor variable, rather than a one-unit increase.

Here we compare the pre and post transformation of coefficients for interpretation: 

```{r}
# all the coefficients
head(coef(simple_logistic))

# for their interpretation
head(exp(coef(simple_logistic)))
```

_FACTOR VARIABLES_

Factor variables when exponentialised give the likelihood to respond no/yes relative to the base group in the factor variable. We use the function `factors` to view the levels and identify the base group.

To assist with interpretation of factor outputs: 

Here is a function to identify all the base levels in each factor group. Useful to refer to during analysis of different logistic models. 

```{r}
extract_base_groups <- function(df) {

  # extract factor vars only and list to print values
  factor_vars <- names(final_data)[sapply(final_data, is.factor)]
  base_groups <- list()
  
  # loop through values to check all levels
  for (var in factor_vars) {
    if (length(levels(df[[var]])) > 0) { # Check step - should be TRUE for all selected. 
      contrast_matrix <- contrasts(df[[var]])
      if (is.null(contrast_matrix)) {
        base_groups[[var]] <- levels(df[[var]])[1] # check only - shouldn't ever run
      } else {
        base_group_level <- levels(df[[var]])[rowSums(abs(contrast_matrix)) == 0] # base group is the one with all zeroes, so we extract that
        base_groups[[var]] <- base_group_level
      }
    } else {
      base_groups[[var]] <- NA # Indicate non-factor or factor with no levels.
    }
  }
  return(base_groups)
}

# run function with 'final data' and print a tidy output
factor_bases <- extract_base_groups(final_data)

factor_bases <- as_tibble(factor_bases) |> 
  pivot_longer(cols = everything(), 
               names_to = "vars",
               values_to = "base_group")

print(factor_bases, n=Inf)
```

##### Model performance - Simple logistic

Here, we test the models ability to classify people as supporting or not supporting the rights. 

```{r}
# AIC of model
simple_logistic$aic

#predicted probabilities of being a 1 (i.e. a refusal of possibility of trans doc)
predicted_probs <- simple_logistic$fitted.values
# same by running
# predicted_probs <- predict(simple_logistic, type = "response")
head(predicted_probs)

# AUC of the model
auc(datalr$trans_docs, predicted_probs)

# Turning them to classes using custom threshold
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
head(predicted_classes)
```

Get the confusion matrix:

```{r}
cM <- confusionMatrix(as.factor(datalr$trans_docs), as.factor(predicted_classes))
cM
```

The simple model predicted people's response with 82% accuracy. That is quite decent for a basic first model. This shows the response is quite explainable with the data we have. The model is slightly better at predicting those who do not support than those who do. Seen by higher specificity than sensitivity (85% vs 77%). But they are both comparable which is good and confirms we don't have great class imbalance. 

Kappa is basically an accurancy score, but taking into account also the possibility of being right by chance. A value of 0.62 is considered quite good.

```{r, message=FALSE}
plot.roc(datalr$trans_docs, predicted_probs, 
         col="darkblue", 
         print.auc = TRUE,  
         auc.polygon=TRUE,
         max.auc.polygon=TRUE,
         auc.polygon.col="lightblue", 
         print.thres="best")
```

This suggests changing the threshold for assigning classes to 0.59

Anyway we are not really interested in the prediction power of this model, but we are more interested in understanding which variables are relevant or not

## Stepwise regression

Chooses the best simple logistic model based on the lowest AIC achievable

```{r}
set.seed(123)
stepAIC <- step(simple_logistic, direction = "both", trace=0)
stepAIC
```

The stepwise regression has 19 of the 29 variables included in its best model. It is quite consistent with what our significant variables were above, so we will not provide further commentary.  Compared to the full model, this reduced model does not provide a statistically significant improvement in fit. But it does give us a more simple model and shows us what individual level variable may be most important overall. 

```{r}
anova(stepAIC, simple_logistic)
```

## Lasso regularization

We still have too many variables for a mixed model. So here we will use Lasso to keep only our most important variables for the mixed models. 

Without this step, the glmer stage is too computationally expensive. 

```{r}
lasso_data <- final_data |> 
  # Doing same preprocessing as done for simple logistic
  drop_na() |> 
  select(-"country") |> 
  mutate(across(where(is.numeric), scale)) |> 
  mutate(trans_docs = as.numeric(trans_docs == "Yes")) 

# Dummifying all levels to see whether some levels are particularly important
lasso_data <- lasso_data %>% 
  dummy_cols(select_columns = names(.)[sapply(., is.factor)],
             remove_selected_columns = TRUE)

# Dropping one dummy to avoid multicollinearity
# I prefer to do this manually so as to be able choose the baseline category
lasso_data <- lasso_data |> 
  select(-c(gender_Man, `community_Large town`,
            `marital_status_Single (9-10 in d7)`, 
            `occupation_Manual workers (15 to 18 in d15a)`, 
            `social_class_The middle class of society`, 
            religion_Catholic, 
            `phone_access_Landline & mobile`, 
            `bill_issues_From time to time`, 
            `internet_use_Everyday/Almost everyday`, 
            `life_sat_Fairly satisfied`, 
            polintr_Medium,
            `left_right_(5 - 6) Centre`,
            ethnic_minority_0,
            skincolor_minority_0,
            religious_minority_0,
            roma_minority_0,
            sexual_minority_0,
            disability_minority_0,
            suffered_discr_0,
            friends_trans_No))

# Convert data into a matrix for glmnet
x_lasso <- lasso_data |> select(-trans_docs) |> as.matrix() 
y_lasso <- lasso_data |> select(trans_docs) |> as.matrix()

set.seed(123)

# Define lamdas
lambda_seq <- 10^seq(-2, -5, length.out = 100)

# Fit the Lasso model (alpha = 1 for Lasso regularization)
lasso_model <- cv.glmnet(x_lasso, y_lasso, family = "binomial", alpha = 1, lambda = lambda_seq)

lasso_model$lambda.1se # The largest lambda within one standard error of lambda.min. 
# This results in a simpler model with fewer selected features.

# Get all the coefficients
coefficients <- coef(lasso_model, s = "lambda.1se")
# Filter for the non-zero ones
non_zero_features <- rownames(coefficients)[which(coefficients != 0)]
non_zero_features
```

Lasso includes variables similar to what was suggested by our initial logistic regression and the subsequent stepwise.

In particular we can see again that:

1) `age` is relevant

2) It matters whether you have friends that belong to minorities (`n_friends_minorities`)

3) Also `antilgbtq_rights` is clearly relevant

4) It matters whether you are discriminatory against minorities (`minority_disci`)

5) `gender` is relevant

6) It matters whether you are a manager (at least compared to the baseline of being a manual worker)

7) It matters whether you are an Orthodox Cristian (at least compared to the baseline being Catholic)

8) `phone_access` has 2 relevant levels compared with the baseline (having landline and mobile phone access)

9) Not being poor seems to matter (`bill_issues_Almost never/never` compared to a baseline of having bill issues from time to time)

10) Not using internet seems to matter (compared to accessing it every day)

11) Not being happy in life seems to matter

12) Being right-wing rather than center seems to matter

13) Being from an ethnic minority seems to matter

14) Having suffered discrimination seems to matter

15) Supporting a third-gender option on civil documents seems to matter

16) Refusing to answer the question on whether you know a transgender person seems to matter

### Final variable selection

From these hints above and general intuition, we will select the following variables as our individual level fixed effects in the mixed level model: 

- `age`

- `gender`

- `religion`: given that the majority of people in our sample are catholic while the most important levels seem to be non_believers, muslims and Orthodox, instead of using all the levels we are going to use these 3 levels

- `occupation`: it is not clear which levels might be relevant so we will not include any of them

- Place you live does not seem to matter (hence no `community` variables in our final model)

- `marital_status` it is not clear which levels are important so we drop this variable

- `social_class` as our descriptive analysis was suggesting that this might be relevant and given that we are not using occupation, we will include all levels of `social_class`

- `phone_access` seems like it might be important, but its interpretation is difficult and I am not sure what this variable can be a proxy of, therefore we exclude it

- `bill_issues` will be recoded as a dummy using the level almost never. This means that those with 1s in our dummy experience difficult paying bills on a sporadic or constant basis. This can easily be interpreted as a proxy of whether a person is poor or not.

- `internet_use` on a similar concept as the previous point, this variable will be recoded as a dummy storing whether one uses internet every day or not. 

- `life_sat`: people who are not satisfied in life (Not very satisfied & Not at all satisfied) seem to be really different from the rest. So a dummy will be created using those 2 levels

- `left_right`: right wing people seem significantly different than center or left wing so a dummy will be created for right wing people

- `ethnic_minority` and `roma_minority` seem quite important. Given that Roma is an ethnic minority we will keep only the former and discard the latter, which would be redundant information

- Whether you have suffered discrimination (`suffered_discri`) matters

- `n_friends_minorites` and `friends_trans` are very similar concepts. `n_friends_minorites` is derived summing the "yes" answers to "do you know a person that belong to this minority x", but excluding the transgender minority whose answers are left as a standalone variable (`friends_trans`). We are going to use directly `friends_trans` as it is more directly related with our target, discarding `n_friends_minorities` that likely captures a similar effect

- `antilgbtq_rights` is clearly a good predictor. As `gender_docs` again captures a similar concept we are going to use only `antilgbtq_rights`

- `minority_disci` it is also closely related to the last point above, but we are going to keep it as a more general score of how discriminatory you are

And these will be our pre-defined fixed effects.

```{r}
# Preparing the dataframe
complete_df <- complete_df |> 
  # Irrelevant variables
  select(-c(years_edu, community, marital_status, occupation,
            nonEU_national, phone_access, polintr, social_alienation,
            gender_docs, n_friends_minorities, 
            n_actions_against_discri, skincolor_minority,
            religious_minority, roma_minority, sexual_minority,
            disability_minority)) |> 
  # Religion
  mutate(non_believer = ifelse(religion == "Non-believers", 1, 0),
    muslim = ifelse(religion == "Muslim", 1, 0),
    orthodox = ifelse(religion == "Orthodox Christian", 1, 0)) |>
  select(-religion) |> 
  # Bill issues
  mutate(poor = ifelse(bill_issues != "Almost never/never", 1, 0)) |> 
  select(-bill_issues) |> 
  # Internet use
  mutate(everyday_internet = ifelse(internet_use == "Everyday/Almost everyday", 1, 0))  |> 
  select(-internet_use) |> 
  # Life satisfaction
  mutate(unhappy = ifelse(life_sat %in% c("Not very satisfied", "Not at all satisfied"), 1, 0)) |> 
  select(-life_sat) |> 
  # Ideology
  mutate(right_wing = ifelse(left_right == "(7 -10) Right", 1, 0)) |> 
  select(-left_right)
```

```{r}
str(complete_df)
```

## Mixed model

Random Intercept Model:

Allows the baseline level (intercept) to vary across groups. Assumes the relationship between predictors and outcome (slope) is the same for all groups

In R syntax: y ~ x + (1|group)

Example: Different schools might have different average test scores (random intercepts), but the effect of study hours on test scores is the same across all schools

<br>

Random Slope Model:

Allows the effect of a predictor (slope) to vary across groups. Can be used with or without random intercepts.

In R syntax: y ~ x + (0+x|group) for random slope only, or y ~ x + (1+x|group) for both random intercept and slope

Example: The effect of study hours on test scores might be stronger in some schools than others (random slopes)

Random intercept models account for different baselines between groups, while random slope models account for different relationships between predictors and outcomes across groups. When both random intercepts and slopes are included, you're allowing both the baseline and the effect of predictors to vary by group.

<br>

Interactions between group-level (higher-level) variables and individual-level (lower-level) variables in mixed models are called cross-level interactions.

Cross-level interactions are particularly useful in multilevel research because they help you understand how the relationship between an individual-level predictor and the outcome varies as a function of a group-level characteristic.

For example, in educational research: 

1) Individual level: student characteristics (study time, prior knowledge). 

2) Group level: school or classroom characteristics (class size, teaching method). 

3) Cross-level interaction: Does the effect of study time on performance depend on class size?

### Running the glmer models

To model, we will follow: Approach to multilevel model building based on Hox (2010)

1/ Null Model (Random Intercept only) 

2/ Add independent Level 1 variables 

3/ Add independent Level 2 variables 

4/ Add random slopes 

5/ (Cross-level) interactions

Each step, check whether your model is significantly improved compared
to the previous one. We are actually aggregating step 4 and 5 due to computational times.

Adjusted ICC: The proportion of total variance explained by the grouping structure (random effects), adjusted to include the variance explained by the fixed effects

Unadjusted ICC: The proportion of total variance explained by the grouping structure (random effects), excluding any variance explained by the fixed effects in the model

#### Mixed model 1 - null model country level random effects only

```{r, message=FALSE}
complete_df <- complete_df |> 
  drop_na() |> # we have NAs only in our target
  mutate(trans_docs = as.numeric(trans_docs == "Yes"),
         ethnic_minority = as.numeric(ethnic_minority) - 1,
         suffered_discr = as.numeric(suffered_discr) - 1) 

# Scaling the variables is necessary otherwise the model cannot run
# However we loose interpretability 
scaled_df <- complete_df |>
  mutate(across(c(age, antilgbtq_rights, minority_discri, gdp_pc_ppp,
                  gender_inequality_index, lgbt_policy_index, 
                  democracy_index), ~scale(.x, center = TRUE, scale = TRUE)))
```

```{r, message=FALSE}
# Null model random intercept only
glmer_step1 <- glmer(trans_docs ~ (1|country),
                          data = scaled_df, 
                          family=binomial(link="logit"))
# Printing coefficients
nice_table(broom.mixed::tidy(glmer_step1))
# Random intercepts coefficients
ranef(glmer_step1)
# AIC
AIC(glmer_step1)
# AUC
predicted_probs <- predict(glmer_step1)
auc(scaled_df$trans_docs, predicted_probs)
# ICC
performance::icc(glmer_step1)
```
The AUC here measures the models ability to determine trans_docs by country only differences. The AUC of 0.7405 shows just using country, you have better than random prediction (which would be 0.6 given that our class balances is 60% Yes and 40% No). 

The ICC represents the proportion of variance in trans_docs attributable to differences between countries. The value of 0.227 shows that country is a significant variable, but indiviudal level varibles must be included to make this a properly good explanatory model. 

The AIC (28212) can be used for model performance later. If AIC reduces, the other models are improvments. 

#### Mixed model 2 - adding in individual-level fixed effects 

```{r, message=FALSE}
# Random intercept + individual fixed effects
glmer_step2 <- glmer(trans_docs ~ age + gender + social_class +
                       ethnic_minority + suffered_discr + 
                       friends_trans + antilgbtq_rights +
                       minority_discri + non_believer + muslim +
                       orthodox + poor + everyday_internet + 
                       unhappy + right_wing + (1|country),
                     data = scaled_df,
                     family = binomial(link="logit"))
# Printing coefficients
nice_table(broom.mixed::tidy(glmer_step2))
# AIC
AIC(glmer_step2)
# AUC
predicted_probs <- predict(glmer_step2)
auc(scaled_df$trans_docs, predicted_probs)
# Turning them to classes using custom threshold
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# ICC
performance::icc(glmer_step2)
# Model comparison
anova(glmer_step1, glmer_step2)
```

After adding in the indiviudal level fixed effects, we see:

The model is improved. We have a significantly reduced AIC (23612) and also see improved AUC (up to around 0.84 to show classification power)

The ICC has reduced, this indicates that the indiviudual level variables are now explaining some of the variance previously explained by the country level variable.

#### Mixed model 3 - adding in country-level fixed effects 

```{r, message=FALSE}
# Random intercept + individual fixed effects + country fixed effects
glmer_step3 <- glmer(trans_docs ~ age + gender + social_class +
                       ethnic_minority + suffered_discr + 
                       friends_trans + antilgbtq_rights +
                       minority_discri + non_believer + muslim +
                       orthodox + poor + everyday_internet + 
                       unhappy + right_wing + gdp_pc_ppp +
                       lgbt_policy_index + gender_inequality_index +
                       democracy_index + (1|country),
                     data = scaled_df,
                     family = binomial(link="logit"))
# Printing coefficients
nice_table(broom.mixed::tidy(glmer_step3))
# AIC
AIC(glmer_step3)
# AUC
predicted_probs <- predict(glmer_step3)
auc(scaled_df$trans_docs, predicted_probs)
# ICC
performance::icc(glmer_step3)
# Model comparison
anova(glmer_step1, glmer_step2, glmer_step3)
```

In this model, when including country level fixed effects too, we see:

An overfitted model compared to step 2 (indiviudal fixed effects). 

A better model compared to step 1 (null).

The model is overfitted because performance and predictive power do not improve comapred to step 2 but we have additional variables. The AUC is equivalent while the AIC has increased (indicating worse overall model fit). Although we are adding complexity the model is not getting better.

#### Mixed model 4 - adding in cross-level interactions and random slopes 

This final mixed model includes cross-level interactions between individual and country level data. The following interactions are included in our models:

`right_wing:lgbt_policy_index`: Tests whether the effect of individual political ideology varies based on a country's LGBT policy environment. The relationship between conservative views and trans attitudes might be weaker in countries with strong LGBT protections.

`social_class:gdp_pc_ppp`: Tests whether social class effects differ across countries with varying economic development.

`gender:gender_inequality_index`: Tests how the relationship between being female and attitudes toward transgender people varies depending on the level of gender inequality in their country.

`gender:lgbt_policy_index`: Tests how the relationship between an individual's gender and their attitudes toward transgender people varies depending on the LGBTQ policy environment of their country.

`age:gdp_pc_ppp`: Tests how the relationship between an individual's age and their attitudes toward transgender people varies depending on the economic development level of their country.

`non_believer:democracy_index`: Tests how the relationship between being religious and attitudes toward transgender people vary depending on the democratic context of their country.

`non_believer:gender_inequality_index`: Tests how the relationship between being religious and attitudes toward transgender people vary depending on the level of gender inequality in their country

`unhappy:lgbt_policy_index`: Tests how the relationship between an individual's unhappiness (or life dissatisfaction) and their attitudes toward transgender people varies depending on the LGBTQ policy environment of their country.
 
`poor:democracy_index`: Tests how the relationship between an individual's economic status (being poor) and their attitudes toward transgender people varies depending on the democratic context of their country.

```{r, message=FALSE}
# Adding random slopes and cross level interaction
glmer_step4 <- glmer(trans_docs ~ 
                       # Individual level variables
                       age + gender + social_class +
                       ethnic_minority + suffered_discr + 
                       friends_trans + antilgbtq_rights +
                       minority_discri + non_believer + muslim +
                       orthodox + poor + everyday_internet + 
                       unhappy + right_wing + 
                       # Country level variables
                       gdp_pc_ppp + gender_inequality_index +
                       lgbt_policy_index + democracy_index + 
                       # Cross-level interactions
                       right_wing:lgbt_policy_index +
                       social_class:gdp_pc_ppp +
                       gender:gender_inequality_index +
                       gender:lgbt_policy_index +
                       age:gdp_pc_ppp +
                       non_believer:democracy_index +
                       non_believer:gender_inequality_index +
                       unhappy:lgbt_policy_index +
                       poor:democracy_index +
                       (1 + gdp_pc_ppp +
                       lgbt_policy_index + gender_inequality_index +
                       democracy_index |country),
                     data = scaled_df,
                     family = binomial(link="logit"))
```

Run the diagnostics: 

```{r, message=FALSE}
# Printing coefficients
nice_table(broom.mixed::tidy(glmer_step4))
# AIC
AIC(glmer_step4)
# AUC
predicted_probs <- predict(glmer_step4)
auc(scaled_df$trans_docs, predicted_probs)
# ICC
performance::icc(glmer_step4)
# Model comparison
anova(glmer_step1, glmer_step2, glmer_step3, glmer_step4)

# Class predictions
predicted_classes <- ifelse(predicted_probs > 0.5, 1, 0)
# Confusion matrix
confusionMatrix(as.factor(scaled_df$trans_docs), as.factor(predicted_classes))
plot.roc(scaled_df$trans_docs, predicted_probs, 
         col="darkblue", 
         print.auc = TRUE,  
         auc.polygon=TRUE,
         max.auc.polygon=TRUE,
         auc.polygon.col="lightblue", 
         print.thres="best")
```

This model, although computationally expensive, is a marginal improvement on the previous best model (model 1, individual fixed effects). The model fit has improved.  We see that in this full model, the AIC dropped to 23606. However, the BIC increased from the model 1, so that does create some uncertainty. 

The predictive power actual increases very very marginally. The AUC increases from 0.8389 to 0.8394 in the full model. This may indicate the predictive power is maximised around 84%, given such minor improvements, the model with only individual level fixed effects is preferred. 

We have an accuracy 0.76. The model is better at identifying negatives than positives since specificity (83.11%) is higher than sensitivity (68.65%), indeed the AUC graph suggests changing the threshold to around 0.4 from 0.5 so as to better balance those two metrics

##### Cross-level effects 

When looking at just the cross-level effects, the significant relationships and the direction of the relationship were: 

right_wing × lgbt_policy_index (negative)

genderWoman × lgbt_policy_index (positive)

non_believer × democracy_index (negative)

non_believer × gender_inequality_index (negative)

These effects are interpreted as the relationships between an individual-level variable and the outcome variable changes depending on the value of a country-level variable.

For the right wing x lgbti policy variable, as the coefficient is negative, we infer that the effect of being right wing is weaker in countries with better lgbti policies. So more mild right wing opposition to transgender documentation occurs in more progressive countries (those with stronger lgbti views). This makes sense and maybe helps capture some of the political opinion subjectivity between individuals in differenet countries. 

For the women x lgbti policy variable, suggests that the effect of being a women in countries with stronger lgbti policies is stronger. Basically meaning that women tend to be more supportive of transgenders' rights in countries with more inclusive lgbtq policies

For non-believer x democracy index and non-believer x gender inequality index, both are negative. This implies that non-religious people are less likely to support transgenders’ rights in countries where there is higher gender inequality. This also implies that the effect of being a non-religious on support for trans rights is weaker in more democratic countries. Although this might sound counterintuitive, this could align with more democratic having lower levels of religiosity, and thus religion in general playing a lower impact on determining whether one supports or not transgenders' people rights. 

### Interpreting results

The best model is the full model with all of the individual-level, country-level and cross-level effects included. However the gains are very marginal from the previous best model. The final model with cross-level effects is very computationally expensive and offers little improvement on the more simple model. So depending on needs, the model 1 may be the better option due to simplicity in interpretation and efficiency of computation. 

It is also worth noting that ICC coefficients are quite low across all the models and that country-level fixed effects are not significant. This suggests that most of the variation is explained at the individual level rather than at the country level.

#### Key variables 

The key individual-level variables in the models that are significant include most of the variables that we had already individuated before.

In particular women, people who use internet every day, people that are non-religious and people that know a transgender person tend to be more supportive. On the contrary males, people from ethnic minorities, unhappy people and right-wing people tend to oppose transgender rights more. Our discriminatory scores also seem always relevant with people being more discriminatory against minorities being less supportive of the right in question proving to be good index of discriminatory attitudes. 

Perhaps counterintuitively, but consistent with our previous findings, age has a positive coefficient meaning that older people tend to be more supportive. Interpretation of this finding is difficult as our bivariate descriptive analysis seems to contradict this.

#### Model classification power

Here we can show the distribution of our preferred model, with random slopes for country and fixed individual effects. 

```{r, warning=FALSE}
# summarise predictions
prediction_summary <- scaled_df |> 
  select(trans_docs, 
         country) |> 
  mutate(m1preds = predict(glmer_step1),
         m1_model_pred = ifelse(m1preds > 0.5, 1, 0),
         m2_preds = predict(glmer_step2),
         m2_model_pred = ifelse(m2_preds > 0.5, 1, 0),
         m3_preds = predict(glmer_step3),
         m3_model_pred = ifelse(m3_preds > 0.5, 1, 0),
         m4_preds = predict(glmer_step4), 
         m4_model_pred = ifelse(m4_preds > 0.5, 1, 0))

# calculate rates for each model at country level and pivot longer
summary_table <- prediction_summary %>%
  select(trans_docs, country, m1_model_pred, m2_model_pred, 
         m3_model_pred, m4_model_pred) |>
  group_by(country) %>%
  summarise(
    actual_rate = mean(trans_docs)*100,
    m1_rate = mean(m1_model_pred)*100,
    m2_rate = mean(m2_model_pred)*100,
    m3_rate = mean(m3_model_pred)*100,
    m4_rate = mean(m4_model_pred)*100)

#plot all of the model estimates on a graph
# with unpivoted data
summary_table %>%
  mutate(
    model_1 = "Model 1 (null model = country random intercept)",
    model_2 = "Model 2 (+individual fixed)",
    model_3 = "Model 3 (+country fixed)",
    model_4 = "Model 4 (+cross-level & country random slope)"
  ) %>%
  ggplot(aes(x = reorder(country, -actual_rate), y = actual_rate)) +
  geom_col(alpha = 0.5) +
  geom_point(aes(y = m1_rate, color = model_1),
             size = 3, shape = 18,
             position = position_nudge(x = -0.3)) +
  geom_point(aes(y = m2_rate, color = model_2),
             size = 3, shape = 15,
             position = position_nudge(x = -0.1)) +
  geom_point(aes(y = m3_rate, color = model_3),
             size = 3, shape = 16,
             position = position_nudge(x = 0.1)) +
  geom_point(aes(y = m4_rate, color = model_4),
             size = 3, shape = 17,
             position = position_nudge(x = 0.3)) +
  scale_color_manual(name = "Mixed model Type",
                       values = c("Model 1 (null model = country random intercept)" = "blue",
                                  "Model 2 (+individual fixed)" = "red",
                                  "Model 3 (+country fixed)" = "green",
                                  "Model 4 (+cross-level & country random slope)" = "purple")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5),
        axis.title.x = element_blank())+
  scale_y_continuous(labels = percent_format(scale = 1)) +
  labs(title = "Comparing explantory mixed models classification power",
       y = "Proportion of support",
       x = "Countries, ordered by actual support") +
  theme_minimal()+
  theme(panel.grid = element_blank(),
        legend.position = c(0.98, 0.98),  # Adjust coordinates for top right
        legend.justification = c("right", "top"),
        legend.background = element_rect(fill = "white", color = "gray"))
```

From this section we can see that the 3 GLM models are very similar. The step 2 (red) and step 3 (green) models are indistinguishable. There is some small difference in country level classification power to the step 4 cross-level effects if you look very closely on some countries. However, these models have similar trends. They classify observations that have a more even split of support. While the countries with more extreme support/opposition provide a class imbalance challenge for the explanatory model, worsening its results.  

# Predictive model

Developing a Predictive Model for Other Countries

1) Summarize individual data to a country level (ex. percentage of population living in rural areas by country, percentage of people from minority x by country). Target variable will become % of people in country x that support the right for transgender people to change their civil documents.

2) Then run random forest/gradient boosting/elastic-net linear regression with leave-one-out cross validation.

Leave-One-Out Cross-Validation (LOOCV):
LOOCV is a special case of k-fold cross-validation, where k = the number of observations (n). That means: For each observation (1 out of 28), the model is trained on the remaining 27 rows and tested on the left-out row. This process repeats 28 times (once per row). The final performance metric (e.g., accuracy, RMSE) is the average of all 28 test results.

We switched to country level predictions because this is as we understood the task.

Given that by switching to country level data we only have 28 observations it does not make sense to split our dataframe into test and training for model comparison. Instead we will exploit the fact of having so few rows to run leave-one-out cross validation for hyperparameter tuning and to compare our models.

It is also worth noting that the generalizability of our model is very low. Partially because of the few observations, partially because it is trained only on data from European Union countries. These countries share many similarities. The models we derive could be used to predict support for `trans_docs` in other European countries, but these models' findings are likely to be extremely wrong for countries outside Europe, or with a socio-economic structure completely different than that of EU countries. 

## Aggregating individual data to country level

We should be building a dataframe with 28 rows (1 per country), summarizing all our variables at the country level.

In the next steps we will synthesize the information from the individual respondents into country-level variables.

Using means for numeric variables:

```{r}
# using final_data which contains individual level data only
names(select(final_data, where(is.numeric))) 

data_num <- final_data |> 
  group_by(country) |> 
  summarise(across(where(is.numeric), ~mean(.), .names = "mean_{.col}"))
```

As well as for binary dummy variables:

```{r, warning=FALSE}
# Searching for factor variables with two levels
dummy_names <- names(final_data)[sapply(final_data, 
                                        is.factor) & 
                                   sapply(final_data, 
                                          function(x) length(levels(x)) == 2)]
dummy_names

dummy_levels <- lapply(final_data[dummy_names], levels)
dummy_levels

data_dummy <- final_data %>%
  select(country, all_of(dummy_names)) %>%
  mutate(across(all_of(dummy_names),
                ~ case_when(
                  # convert "0"/"1" to 0/1
                  . %in% c("0", "1") ~ as.numeric(as.character(.)),
                  # convert "Yes"/"No" to 1/0
                  . %in% c("Yes", "No") ~ recode(., "Yes" = 1, "No" = 0),
                  # convert "Man"/"Woman" to 1/0
                  . %in% c("Man", "Woman") ~ recode(., "Man" = 1, "Woman" = 0), 
                  TRUE ~ NA 
                ),
                .names = "{.col}_num"))
```

We convert factors with 2 levels to numerical 0-1 so we can compute their means.

```{r}
# Country means
data_dummy <- data_dummy %>%
  group_by(country) %>%
  summarise(
    across(ends_with("_num"), ~ mean(.x, na.rm = TRUE)),
    .groups = "drop"
  )

# substitute the "_num" suffix with a "mean_" prefix so it has the same format as the variables in data_num
data_dummy <- data_dummy %>%
  rename_with(~ paste0("mean_", str_remove(., "_num$")), ends_with("_num"))
```

Now we can join the two datasets.

```{r}
data_aggr <- data_num %>%
  left_join(data_dummy, by = "country")
```

The next variables are factor with >2 levels:

```{r}
factor_names <- names(final_data)[!sapply(final_data, is.numeric) &
                                  !(sapply(final_data, is.factor) & 
                                      sapply(final_data, 
                                             function(x) length(levels(x)) == 2))]
factor_names

# checking the levels
lapply(final_data[factor_names], levels)

# removing country from the list
factor_names <- setdiff(factor_names, "country")

# creating the aggregated dataset for factor variables
data_factors <- final_data %>%
  select(country, all_of(factor_names)) %>%
  pivot_longer(cols = -country, names_to = "variable", values_to = "level") %>%
  group_by(country, variable, level) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(country, variable) %>%
  mutate(proportion = count / sum(count)) %>%
  select(-count) %>%
  pivot_wider(names_from = c(variable, level), values_from = proportion, names_glue = "{variable}_{level}")

colSums(is.na(data_factors))
```

We are seeing NAs because in not all countries are all the factors' levels represented. For example we see that `social_class_The higher class of society` has 3 missing values. Let's confirm that there are 3 countries in our dataframe with no observation for that level

```{r}
final_data |> filter(social_class=="The higher class of society") |>  pull(country) |> unique() |> length()
```

There are indeed 3 countries with non observation of `social_class=="The higher class of society"`. Therefore those percentages should be 0s and not NAs.

```{r}
data_factors <- data_factors |>
  mutate(across(everything(), ~replace_na(., 0)))
```

Joining all together

```{r}
data_aggr <- data_aggr |> 
  left_join(data_factors, by = "country")
```

Lastly, joining the country level variables obtained from external sources

```{r}
data_aggr <- data_aggr |> 
  left_join(country_level_data, by = c("country" = "iso2c")) |> 
  select(-c(iso3c, country.y))
```

```{r}
dim(data_aggr)
```

The resulting data frame has 28 observations and 79 variables. 

## Linear regression with Elastic Net

Model output compared below in predictive model performance section. 

```{r}
data_models <- data_aggr |> 
  select(-country)

set.seed(123)

# Define the leave-one-out cross-validation control
ctrl <- trainControl(
  method = "LOOCV",
)

# Defining Elastic net grid 
elastic_grid <- expand.grid(
  alpha = seq(0, 1, length = 10),  # Search from Ridge (0) to Lasso (1)
  lambda = 10^seq(-5, 1, length = 100)  # Regularization strength
)

# Train the Elastic Net model
elastic_net_model <- train(
  mean_trans_docs ~ ., 
  data = data_models, 
  method = "glmnet",
  preProcess = c("center", "scale"),
  trControl = ctrl,
  tuneGrid = elastic_grid,
  metric = "RMSE"
)

# Display the best alpha and lambda values
elastic_net_model$bestTune
```

## Random forest

Model output compared below in predictive model performance section. 

```{r, message=FALSE}
set.seed(123)

# Define LOOCV control
control <- trainControl(
  method = "LOOCV",
  returnResamp = "all"
)

# Define hyperparameter grid for Random Forest
rf_grid <- expand.grid(
  mtry = seq(2, ncol(data_models) - 1, by = 2)
)
# mtry the number of variables randomly sampled at each split

# Train Random Forest model with hyperparameter tuning
rf_model <- train(
  mean_trans_docs ~ ., 
  data = data_models, 
  method = "rf",
  preProc=c('scale','center'),
  trControl = control,
  tuneGrid = rf_grid,
  importance = TRUE   # Calculate variable importance
)

# Results
plot(rf_model)
rf_model$bestTune
```


## Gradient boosting

Model output compared below in predictive model performance section. 

```{r, message=FALSE}
set.seed(123)

control <- trainControl(
  method = "LOOCV",
  returnResamp = "all"
)

gbm_grid <- expand.grid(
  n.trees = c(50, 100, 200),           # Number of trees
  interaction.depth = c(1, 2, 3),      # Tree depth
  shrinkage = c(0.01, 0.05, 0.1),      # Shrinkage
  n.minobsinnode = c(1, 3, 5)          # Minimum observations
)

gbm_model <- train(
  mean_trans_docs ~ ., 
  data = data_models, 
  method = "gbm",
  preProc = c('scale', 'center'),
  trControl = control,
  tuneGrid = gbm_grid,
  verbose = FALSE)

gbm_model$bestTune
```

## Predictive model performance 

Here we analyse the benchmark model and 3 predictive models. We aggregate all the performance metrics in this section and provide commentary afterwards on what the best model is. 

```{r}
# Adding benchmark (mean)
benchFit <- lm(mean_trans_docs ~ 1, data=data_models)

benchmark_pred <- predict(benchFit, data_aggr)
elastic_pred <- predict(elastic_net_model, data_aggr)
rf_pred <- predict(rf_model, data_aggr)
gbm_pred <- predict(gbm_model, data_aggr)

# Actual values
actual <- data_aggr$mean_trans_docs

# Create a data frame with all predictions and actual values
all_predictions <- data.frame(
  country = data_aggr$country,
  actual = actual,
  benchmark = benchmark_pred,
  elastic_net = elastic_pred,
  random_forest = rf_pred,
  gradient_boosting = gbm_pred
)

# Calculate errors for each model
all_predictions$benchmark_error <- abs(all_predictions$actual - all_predictions$benchmark)
all_predictions$elastic_error <- abs(all_predictions$actual - all_predictions$elastic_net)
all_predictions$rf_error <- abs(all_predictions$actual - all_predictions$random_forest)
all_predictions$gbm_error <- abs(all_predictions$actual - all_predictions$gradient_boosting)

# Calculate performance metrics for each model
calculate_metrics <- function(actual, predicted) {
  residuals <- actual - predicted
  r_squared <- 1 - sum(residuals^2) / sum((actual - mean(actual))^2)
  rmse <- sqrt(mean(residuals^2))
  mae <- mean(abs(residuals))
  
  return(c(R_squared = r_squared, RMSE = rmse, MAE = mae))
}

# Get metrics for each model
benchmark_metrics <- calculate_metrics(actual, benchmark_pred)
elastic_metrics <- calculate_metrics(actual, elastic_pred)
rf_metrics <- calculate_metrics(actual, rf_pred)
gbm_metrics <- calculate_metrics(actual, gbm_pred)

# Combine metrics
model_metrics <- rbind(
  "Benchmark(=Mean)" = benchmark_metrics,
  Elastic_Net = elastic_metrics,
  Random_Forest = rf_metrics,
  Gradient_Boosting = gbm_metrics
)

# Print performance comparison
print(model_metrics)
```

By using just the mean of our target variable as a prediction we can see that the mean absolute error is 0.17. It means that by using the mean we are going to be off by 17% points on average, this provides a really good benchmark to understand the results. 

<br>

We are able to predict so good partially because this is a set of observations which share many characteristics among them. Furthermore, due to the low number of observations in our data we are able to run LOOCV which yields amazing results. However, we should treat these results carefully, as the limited number of observation and such high performance metrics suggest that our models are prone to overfitting. Again this is mostly due to having few and similar observations, hence we are able to predict really well among those observations, but the generizability power of our model is quite low. We would need many more and many more diverse countries in our training dataset to build a model which could be applicable outside of Europe.

```{r}
# Sort predictions by actual values (descending)
sorted_predictions <- all_predictions[order(all_predictions$actual, decreasing = TRUE), ]

# Print sorted predictions
print(sorted_predictions[, c("country", "actual", "elastic_net", "random_forest", "gradient_boosting")])
```

It seems that random forests and elastic net struggle more in predicting extreme values of our target variable. The range of true values of our target variable goes from 15% to 90%, elastic net predictions only range within 20% and 85%, while random forests' predictions even worse, they range between 25% and 82%. Gradient boosting instead is able to predict better those extreme values and it seems that in this lies its better predictive performance.

```{r, warning=FALSE}
#plot all of the model estimates on a graph
ggplot(sorted_predictions, aes(x = reorder(country, -actual), 
                               y = actual)) +
  geom_col(alpha = 0.5) +
  geom_point(aes(y = elastic_net , color = "Elastic Net"),
             size = 3, shape = 15,
             position = position_nudge(x = -0.1)) +
  geom_point(aes(y = random_forest, color = "Random Forest"),
             size = 3, shape = 16,
             position = position_nudge(x = 0.1)) +
  geom_point(aes(y = gradient_boosting, color = "Gradient Boosting"),
             size = 3, shape = 17,
             position = position_nudge(x = 0.3)) +
  geom_hline(aes(yintercept = mean(actual), 
                 color = "Benchmark (mean)"), 
           linetype = "dashed", linewidth = 0.7) +
  scale_color_manual(name = "Model Type",
                   values = c("Benchmark (mean)" = "black",
                              "Elastic Net" = "red",
                              "Random Forest" = "green",
                              "Gradient Boosting" = "purple")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 0.5),
        axis.title.x = element_blank())+
  scale_y_continuous(labels = percent_format(scale = 1)) +
  labs(title = "Comparing predictive models regression results",
       y = "Proportion of support",
       x = "Countries, ordered by actual support") +
  theme_minimal()+
  theme(panel.grid = element_blank(),
        legend.position = c(0.98, 0.98),
        legend.justification = c("right", "top"),
        legend.background = element_rect(fill = "white", color = "gray"))
```

We can see again how gradient boosting is almost always perfect, while random forest and elastic net alternate themselves for the second best moel depending on the country.

```{r}
# Reshape the data for plotting
plot_data <- sorted_predictions %>%
  select(country, actual, benchmark, elastic_net, random_forest, gradient_boosting) %>%
  pivot_longer(cols = -c(country, actual, benchmark), names_to = "model", values_to = "prediction")

# Create a scatterplot of actual vs predicted for each model
ggplot(plot_data, aes(x = actual, y = prediction, color = model)) +
  geom_point(size = 3, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  facet_wrap(~model) +
  theme_minimal() +
  labs(title = "Actual vs Predicted Values by Model",
       x = "Actual Values", y = "Predicted Values")

# Find which model performed best for each country
all_predictions$best_model <- apply(all_predictions[, c("elastic_error", "rf_error", "gbm_error")], 1, 
                                   function(x) c("Elastic Net", "Random Forest", "Gradient Boosting")[which.min(x)])

# Count how many countries each model performed best on
table(all_predictions$best_model)
```

Overall, the gradient boosting is the best models. The gradient boost model predicts better at the extreme values however i.e. countries with very high or very low support for the target variable. This is because of the class imbalance at these levels. 

The random forest on the other hand, does not predict the more extreme country values as well. 

### Checks for overfitting in GBM

```{r}
# Calculate residuals
gbm_residuals <- actual - gbm_pred

# Create a dataframe
residuals_df <- data.frame(actual, gbm_residuals)

# Plot residuals
ggplot(residuals_df, aes(x = actual, y = gbm_residuals)) +
  geom_point(color = "red", alpha = 0.7) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal() +
  labs(title = "Residuals Plot for Gradient Boosting",
       x = "Actual Values", y = "Residuals")
```

## Variables importance

Commentary for all 3 models provided after the three plots. 

Elastic Net:

```{r}
# Extracting all coefficients of the best Elastic Net Model
coefficients <- coef(elastic_net_model$finalModel, s=elastic_net_model$bestTune$lambda)

# Printing all the non 0s
non0_coefficients <- data.frame(
  feature = rownames(coefficients),
  coefficient = as.numeric(coefficients))  |>  
  filter(coefficient != 0) |> 
  arrange(desc(abs(coefficient)))
non0_coefficients
```

Random Forest:

```{r}
# Plotting the top 10 most important variables 
varImp(rf_model, scale = F)$importance %>%
  as.data.frame() %>%
  tibble::rownames_to_column("variable") %>%
  arrange(desc(Overall)) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = reorder(variable, Overall), y = Overall)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = "Feature Importance - Random Forest",
       x = "Features", y = "Importance") +
    theme_minimal()
```

Gradient Boosting:

```{r}
# Plotting the top 10 most important variables 
varImp(gbm_model, scale = F)$importance %>%
  as.data.frame() %>%
  tibble::rownames_to_column("variable") %>%
  arrange(desc(Overall)) %>%
  slice_head(n = 10) %>%
  ggplot(aes(x = reorder(variable, Overall), y = Overall)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    labs(title = "Feature Importance - Gradient Boosting",
       x = "Features", y = "Importance") +
    theme_minimal()
```

The variables that appear among the top 10 for importance in both random forest and gradient boosting include `gender_docs`, `antilgbtq_rights`, `roma_minority`, `occupation_Manual workers`, `lgbt_policy_index`, `left_right_(7-10) Right`, `mean_years_edu`. Of them, the first three also appear in the non-zero elastic net coefficients. They are consisted with what we had found before throughout all the modelling process. We opted to use only `antilgbtq_rights` and not `gender_docs` in the mixed models and we decided to use `ethnic_minority` instead of `roma_minority`, although it seems that we might have been better suited by the latter.

These predictive models are able to extract valuable information from country-level indexes that were not significant in our predictive models such as `democracy_index` and `lgbt_policy_index`. They also suggests us that we should have maybe have included occupation as its level manual workers seems to be quite important. 

Lastly machine learning methods (random forest and gradient boosting) make a positive use of `years_edu`, which is instead non relevant in Elastic Net and in all our previous model. This suggests us that the relationship between this variable and the target might not be linear and that to extract valuable information from this variable it would have been necessary to do some more feature engineering on it.

Nonetheless, overall all the most relevant variables are coherent with theory and expectations and indeed many of them overlap with our previous findings. 

# Conclusions

After extensive variable selection, cleaning, feature-engineering and feature selection we were able to detect many of the most relevant variables in explaining differences among support levels for the right of transgender to change their gender on civil documents. 

We built several logistic model culminating with a comprehensive logistic mixed model taking into account individual fixed effects, country fixed effects and cross level interactions. This model was only slightly better than much more computationally efficient and simpler models as most of the variance seemed to be explained through individual fixed effects. Nonetheless, it helped us gain even deeper insights into the nuances of support for transgenders' rights.

We then aggregated all our variables to a country level and built predictive models to forecast support for the right to change gender on civil documents and obtained extremely accurate predictive models. Due to the low number of observations in our dataset, our models benefited from LOOCV, leading to impressive predictive performance. However, this also raises concerns about overfitting and limited generalizability beyond the European context. A larger and more diverse dataset would be necessary to enhance robustness and applicability.

Across all the models, in summary, some factors that were related to supporting the policy included: 

- being a women

- knowing a transgender person

- being non-religious

- being an everyday internet user, 

While some of the factors related to opposing the policy included: 

- expressing anti-LGBTQ+ views

- expressing more discriminatory views against minorities, 

- having lower life satisfaction 

- expressing a more right wing ideology. 

We believe that most of these are logical connections to support a more progressive inclusion policy. 
